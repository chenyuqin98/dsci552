{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Generative Models for Text\n",
    "\n",
    "(a) In this problem, we are trying to build a generative model to mimic the writ\u0002ing style of prominent British Mathematician, Philosopher, prolific writer, and\n",
    "political activist, Bertrand Russell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Download the following books from Project Gutenberg http://www.gutenberg.\n",
    "org/ebooks/author/355 in text format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# They are already in data/books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) LSTM: Train an LSTM to mimic Russell’s style and thoughts:\n",
    "\n",
    "i. Concatenate your text files to create a corpus of Russell’s writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read book: TPP.txt, string length: 244306\n",
      "Read book: MLOE.txt, string length: 412226\n",
      "Read book: AIIMAT.txt, string length: 746219\n",
      "Read book: THWP.txt, string length: 2005566\n",
      "Read book: TAMatter.txt, string length: 766542\n",
      "Read book: TAM.txt, string length: 514652\n",
      "Read book: OKEWFSMP.txt, string length: 405741\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def createCorpus(data_dir):\n",
    "    corpus = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for f in files:\n",
    "            with open(root + f, encoding='ascii', errors='ignore') as book:\n",
    "                # read all string in a txt \n",
    "                cur_corpus = book.read().lower()\n",
    "                corpus.append(cur_corpus)\n",
    "            print('Read book: {}, string length: {}'.format(f, len(corpus[-1])))\n",
    "    return corpus\n",
    "\n",
    "concatCorpus = createCorpus('../data/books/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ii. Use a character-level representation for this model by using extended ASCII\n",
    "that has N = 256 characters. Each character will be encoded into a an integer\n",
    "using its ASCII code. Rescale the integers to the range [0, 1], because LSTMuses a sigmoid activation function. LSTM will receive the rescaled integers\n",
    "as its input.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '<': 29, '=': 30, '>': 31, '?': 32, '[': 33, '\\\\': 34, ']': 35, '^': 36, '_': 37, 'a': 38, 'b': 39, 'c': 40, 'd': 41, 'e': 42, 'f': 43, 'g': 44, 'h': 45, 'i': 46, 'j': 47, 'k': 48, 'l': 49, 'm': 50, 'n': 51, 'o': 52, 'p': 53, 'q': 54, 'r': 55, 's': 56, 't': 57, 'u': 58, 'v': 59, 'w': 60, 'x': 61, 'y': 62, 'z': 63, '{': 64, '|': 65, '}': 66, '~': 67} {'\\n': 0.0, ' ': 0.014925373134328358, '!': 0.029850746268656716, '\"': 0.04477611940298507, '#': 0.05970149253731343, '$': 0.07462686567164178, '%': 0.08955223880597014, '&': 0.1044776119402985, \"'\": 0.11940298507462686, '(': 0.13432835820895522, ')': 0.14925373134328357, '*': 0.16417910447761194, '+': 0.1791044776119403, ',': 0.19402985074626866, '-': 0.208955223880597, '.': 0.22388059701492538, '/': 0.23880597014925373, '0': 0.2537313432835821, '1': 0.26865671641791045, '2': 0.2835820895522388, '3': 0.29850746268656714, '4': 0.31343283582089554, '5': 0.3283582089552239, '6': 0.34328358208955223, '7': 0.3582089552238806, '8': 0.373134328358209, '9': 0.3880597014925373, ':': 0.40298507462686567, ';': 0.417910447761194, '<': 0.43283582089552236, '=': 0.44776119402985076, '>': 0.4626865671641791, '?': 0.47761194029850745, '[': 0.4925373134328358, '\\\\': 0.5074626865671642, ']': 0.5223880597014925, '^': 0.5373134328358209, '_': 0.5522388059701493, 'a': 0.5671641791044776, 'b': 0.582089552238806, 'c': 0.5970149253731343, 'd': 0.6119402985074627, 'e': 0.6268656716417911, 'f': 0.6417910447761194, 'g': 0.6567164179104478, 'h': 0.6716417910447762, 'i': 0.6865671641791045, 'j': 0.7014925373134329, 'k': 0.7164179104477612, 'l': 0.7313432835820896, 'm': 0.746268656716418, 'n': 0.7611940298507462, 'o': 0.7761194029850746, 'p': 0.7910447761194029, 'q': 0.8059701492537313, 'r': 0.8208955223880597, 's': 0.835820895522388, 't': 0.8507462686567164, 'u': 0.8656716417910447, 'v': 0.8805970149253731, 'w': 0.8955223880597015, 'x': 0.9104477611940298, 'y': 0.9253731343283582, 'z': 0.9402985074626866, '{': 0.9552238805970149, '|': 0.9701492537313433, '}': 0.9850746268656716, '~': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def charRepresent(corpus):\n",
    "    chars = set()\n",
    "    for book in corpus:\n",
    "        cur = set(book)\n",
    "        # print(cur)\n",
    "        chars.update(cur)\n",
    "        # print(chars)\n",
    "    chars = sorted(list(chars))\n",
    "    # Rescale the integers to the range [0, 1]\n",
    "    n = len(chars)-1\n",
    "    char2int = dict((c, i) for i, c in enumerate(chars))\n",
    "    scaled_char2int = dict((c, i/n) for i, c in enumerate(chars))\n",
    "    int2char = dict((i, c) for i, c in enumerate(chars))\n",
    "    return char2int, scaled_char2int, int2char\n",
    "char2int, scaled_char2int, int2char = charRepresent(concatCorpus)\n",
    "print(char2int, scaled_char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "iii. Choose a window size, e.g., W = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:40<00:00,  5.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def windowCorpus(corpus, win_size):\n",
    "    input = []\n",
    "    output = []\n",
    "    for w in range(0, len(corpus)-win_size+1, 1):\n",
    "        seqIn = corpus[w : w + win_size - 1]\n",
    "        seqOut = corpus[w + win_size - 1]\n",
    "        input.append([scaled_char2int[c] for c in seqIn])\n",
    "        output.append(char2int[seqOut])\n",
    "    return input, output\n",
    "\n",
    "def dataGenerate(corpus):\n",
    "  win_size = 100\n",
    "  inSeq, outChar = [], []\n",
    "  for book in tqdm(corpus):\n",
    "    cur_in, cur_out = windowCorpus(book, win_size)\n",
    "    inSeq.extend(cur_in)\n",
    "    outChar.extend(cur_out)\n",
    "  return inSeq, outChar\n",
    "\n",
    "inSeq, outChar = dataGenerate(concatCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "iv. Inputs to the network will be the first W W 1 = 99 characters of each sequence,\n",
    "and the output of the network will be the Wth character of the sequence.\n",
    "Basically, we are training the network to predict each character using the 99\n",
    "characters that precede it. Slide the window in strides of S = 1 on the text.\n",
    "For example, if W = 5 and S = 1 and we want to train the network with the\n",
    "sequence ABRACADABRA, The first input to the network will be ABRA\n",
    "and the corresponding output will be C. The second input will be BRAC and\n",
    "the second output will be A, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "v. Note that the output has to be encoded using a one-hot encoding scheme with\n",
    "N = 256 (or less) elements. This means that the network reads integers, but\n",
    "outputs a vector of N = 256 (or less) elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lstm_input = np.reshape(inSeq, (len(inSeq), 99, 1))\n",
    "lstm_output = np.eye(len(char2int))[outChar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5094559, 99, 1) (5094559, 68)\n"
     ]
    }
   ],
   "source": [
    "print(lstm_input.shape, lstm_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi. Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "\n",
    "vii. Use a Softmax output layer to yield a probability prediction for each of the characters between 0 and 1. This is actually a character classification problem with N classes. Choose log loss (cross entropy) as the objective function for the network (research what it means).3\n",
    "\n",
    "viii. We do not use a test dataset. We are using the whole training dataset to\n",
    "learn the probability of each character in a sequence. We are not seeking for\n",
    "a very accurate model. Instead we are interested in a generalization of the\n",
    "dataset that can mimic the gist of the text.\n",
    "\n",
    "ix. Choose a reasonable number of epochs4\n",
    "for training, considering your computational power (e.g., 30, although the network will need more epochs to yield\n",
    "a better model).\n",
    "\n",
    "x. Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross entropy can measure the difference between probality distributions. According to information theory, low probabity event has more information. Information of events can be caculated for given the probabity of events, in our LSTM model, we use softmax to generate probabities, then we use cross entropy loss function to evaluate the distance between model outputs and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 68)                19040     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 68)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 68)                4692      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,732\n",
      "Trainable params: 23,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.7749\n",
      "Epoch 00001: loss improved from inf to 2.77486, saving model to ./checkpoints/LSTM-best-weights-01-2.77.hdf5\n",
      "9951/9951 [==============================] - 1451s 146ms/step - loss: 2.7749\n",
      "Epoch 2/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.6229\n",
      "Epoch 00002: loss improved from 2.77486 to 2.62287, saving model to ./checkpoints/LSTM-best-weights-02-2.62.hdf5\n",
      "9951/9951 [==============================] - 1453s 146ms/step - loss: 2.6229\n",
      "Epoch 3/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.5584\n",
      "Epoch 00003: loss improved from 2.62287 to 2.55835, saving model to ./checkpoints/LSTM-best-weights-03-2.56.hdf5\n",
      "9951/9951 [==============================] - 1456s 146ms/step - loss: 2.5584\n",
      "Epoch 4/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.5131\n",
      "Epoch 00004: loss improved from 2.55835 to 2.51309, saving model to ./checkpoints/LSTM-best-weights-04-2.51.hdf5\n",
      "9951/9951 [==============================] - 1471s 148ms/step - loss: 2.5131\n",
      "Epoch 5/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4764\n",
      "Epoch 00005: loss improved from 2.51309 to 2.47643, saving model to ./checkpoints/LSTM-best-weights-05-2.48.hdf5\n",
      "9951/9951 [==============================] - 1458s 147ms/step - loss: 2.4764\n",
      "Epoch 6/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4461\n",
      "Epoch 00006: loss improved from 2.47643 to 2.44605, saving model to ./checkpoints/LSTM-best-weights-06-2.45.hdf5\n",
      "9951/9951 [==============================] - 1437s 144ms/step - loss: 2.4461\n",
      "Epoch 7/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4199\n",
      "Epoch 00007: loss improved from 2.44605 to 2.41989, saving model to ./checkpoints/LSTM-best-weights-07-2.42.hdf5\n",
      "9951/9951 [==============================] - 1402s 141ms/step - loss: 2.4199\n",
      "Epoch 8/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3979\n",
      "Epoch 00008: loss improved from 2.41989 to 2.39794, saving model to ./checkpoints/LSTM-best-weights-08-2.40.hdf5\n",
      "9951/9951 [==============================] - 1408s 142ms/step - loss: 2.3979\n",
      "Epoch 9/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3801\n",
      "Epoch 00009: loss improved from 2.39794 to 2.38012, saving model to ./checkpoints/LSTM-best-weights-09-2.38.hdf5\n",
      "9951/9951 [==============================] - 1412s 142ms/step - loss: 2.3801\n",
      "Epoch 10/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3646\n",
      "Epoch 00010: loss improved from 2.38012 to 2.36460, saving model to ./checkpoints/LSTM-best-weights-10-2.36.hdf5\n",
      "9951/9951 [==============================] - 1425s 143ms/step - loss: 2.3646\n",
      "Epoch 11/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3511\n",
      "Epoch 00011: loss improved from 2.36460 to 2.35107, saving model to ./checkpoints/LSTM-best-weights-11-2.35.hdf5\n",
      "9951/9951 [==============================] - 1403s 141ms/step - loss: 2.3511\n",
      "Epoch 12/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3366\n",
      "Epoch 00012: loss improved from 2.35107 to 2.33658, saving model to ./checkpoints/LSTM-best-weights-12-2.34.hdf5\n",
      "9951/9951 [==============================] - 1413s 142ms/step - loss: 2.3366\n",
      "Epoch 13/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3223\n",
      "Epoch 00013: loss improved from 2.33658 to 2.32230, saving model to ./checkpoints/LSTM-best-weights-13-2.32.hdf5\n",
      "9951/9951 [==============================] - 1415s 142ms/step - loss: 2.3223\n",
      "Epoch 14/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3403\n",
      "Epoch 00014: loss did not improve from 2.32230\n",
      "9951/9951 [==============================] - 1378s 138ms/step - loss: 2.3403\n",
      "Epoch 15/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3207\n",
      "Epoch 00015: loss improved from 2.32230 to 2.32070, saving model to ./checkpoints/LSTM-best-weights-15-2.32.hdf5\n",
      "9951/9951 [==============================] - 1436s 144ms/step - loss: 2.3207\n",
      "Epoch 16/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3247\n",
      "Epoch 00016: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1407s 141ms/step - loss: 2.3247\n",
      "Epoch 17/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.3333\n",
      "Epoch 00017: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1389s 140ms/step - loss: 2.3333\n",
      "Epoch 18/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.5085\n",
      "Epoch 00018: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1390s 140ms/step - loss: 2.5085\n",
      "Epoch 19/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.7923\n",
      "Epoch 00019: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1399s 141ms/step - loss: 2.7923\n",
      "Epoch 20/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.6707\n",
      "Epoch 00020: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1395s 140ms/step - loss: 2.6707\n",
      "Epoch 21/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.6042\n",
      "Epoch 00021: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1391s 140ms/step - loss: 2.6042\n",
      "Epoch 22/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.5505\n",
      "Epoch 00022: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1389s 140ms/step - loss: 2.5505\n",
      "Epoch 23/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4998\n",
      "Epoch 00023: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1390s 140ms/step - loss: 2.4998\n",
      "Epoch 24/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4898\n",
      "Epoch 00024: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1393s 140ms/step - loss: 2.4898\n",
      "Epoch 25/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.6754\n",
      "Epoch 00025: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1396s 140ms/step - loss: 2.6754\n",
      "Epoch 26/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.6539\n",
      "Epoch 00026: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1395s 140ms/step - loss: 2.6539\n",
      "Epoch 27/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4898\n",
      "Epoch 00027: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1397s 140ms/step - loss: 2.4898\n",
      "Epoch 28/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4372\n",
      "Epoch 00028: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1394s 140ms/step - loss: 2.4372\n",
      "Epoch 29/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.4227\n",
      "Epoch 00029: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1398s 140ms/step - loss: 2.4227\n",
      "Epoch 30/30\n",
      "9951/9951 [==============================] - ETA: 0s - loss: 2.6236\n",
      "Epoch 00030: loss did not improve from 2.32070\n",
      "9951/9951 [==============================] - 1378s 138ms/step - loss: 2.6236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4b83944c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# build LSTM model\n",
    "LSTMmodel = Sequential()\n",
    "LSTMmodel.add(LSTM(68, input_shape=(99, 1)))\n",
    "LSTMmodel.add(Dropout(0.2))\n",
    "LSTMmodel.add(Dense(68, activation='softmax'))\n",
    "print(LSTMmodel.summary())\n",
    "LSTMmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# save checkpoint\n",
    "filepath = \"./checkpoints/LSTM-best-weights-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# train model\n",
    "LSTMmodel.fit(lstm_input, lstm_output, epochs=30, batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "xi. Use the network with the best weights to generate 1000 characters, using the\n",
    "following text as initialization of the network:\n",
    "\n",
    "There are those who take mental phenomena naively, just as they\n",
    "would physical phenomena. This school of psychologists tends not to\n",
    "emphasize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel.load_weights('./checkpoints/LSTM-best-weights-15-2.32.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.17it/s]There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. \n",
      "the soace of the soace of the soace of the soace of the soace of the \n",
      "aeloe the soace of the porpent of the porpint of the soace of the soace of the soace of the \n",
      "aelos thete oe the porpint of the porpent of the soace of the soace of the soace of the soace \n",
      "the soace of the porpent of the porpent of the porpint of the soace of the soace of the soace of \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#######{ 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\(4\n",
      "\n",
      "\n",
      "\\\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\(9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\48\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 \n",
      "\n",
      "\n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\1\"\n",
      "1111\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "1\n",
      "1 1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "}(1(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "911\n",
      "\n",
      "\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "115\n",
      "\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\(\\\n",
      "\n",
      "\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\1\"\n",
      "\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\\(1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "~1(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ") \n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ 4\n",
      "\n",
      "\n",
      "\n",
      ")\\(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      "\n",
      "\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "1\\\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "111\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\\\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "txt = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "\n",
    "LSTM_input_seq = [scaled_char2int[c] for c in txt[-99:].lower()]\n",
    "for i in tqdm(range(1000)):\n",
    "  seq = np.reshape(LSTM_input_seq, (1, len(LSTM_input_seq), 1))\n",
    "  # predict the next character\n",
    "  predictChar = LSTMmodel.predict(seq)\n",
    "  predictIdx = np.argmax(predictChar) \n",
    "  txt += int2char[predictIdx]\n",
    "  # make new input sequence\n",
    "  LSTM_input_seq.append(predictIdx/(len(char2int)-1))\n",
    "  LSTM_input_seq = LSTM_input_seq[1:len(LSTM_input_seq)]\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "652856dbc3ab2a5d901ecb0bf4a112f6ee4b44e9af7f37c5c2559b5777fde759"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

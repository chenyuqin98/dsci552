{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Supervised, Semi-Supervised, and Unsupervised Learning\n",
    "\n",
    "(a) Download the Breast Cancer Wisconsin (Diagnostic) Data Set from:\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\n",
    "%28Diagnostic%29. Download the data in https://archive.ics.uci.edu/ml/\n",
    "machine-learning-databases/breast-cancer-wisconsin/wdbc.data, which\n",
    "has IDs, classes (Benign=B, Malignant=M), and 30 attributes. This data has\n",
    "two output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0   1      2      3       4       5        6        7        8   \\\n",
       "0      842302   1  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517   1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903   1  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301   1  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402   1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ...  ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424   1  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682   1  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954   1  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241   1  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751   0   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../data/Homework 6 Data/wdbc.data', header=None)\n",
    "data.replace('M', 1, inplace=True)\n",
    "data.replace('B', 0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(b) use 20% of both the positve and nega\u0002tive classes as the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>10.440</td>\n",
       "      <td>15.46</td>\n",
       "      <td>66.62</td>\n",
       "      <td>329.6</td>\n",
       "      <td>0.10530</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>0.012160</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>0.06450</td>\n",
       "      <td>...</td>\n",
       "      <td>11.52</td>\n",
       "      <td>19.80</td>\n",
       "      <td>73.47</td>\n",
       "      <td>395.4</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>0.11530</td>\n",
       "      <td>0.02639</td>\n",
       "      <td>0.04464</td>\n",
       "      <td>0.2615</td>\n",
       "      <td>0.08269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>13.210</td>\n",
       "      <td>25.25</td>\n",
       "      <td>84.10</td>\n",
       "      <td>537.9</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.05205</td>\n",
       "      <td>0.027720</td>\n",
       "      <td>0.020680</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.05584</td>\n",
       "      <td>...</td>\n",
       "      <td>14.35</td>\n",
       "      <td>34.23</td>\n",
       "      <td>91.29</td>\n",
       "      <td>632.9</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.06005</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.06788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>11.710</td>\n",
       "      <td>17.19</td>\n",
       "      <td>74.68</td>\n",
       "      <td>420.3</td>\n",
       "      <td>0.09774</td>\n",
       "      <td>0.06141</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.032390</td>\n",
       "      <td>0.1516</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01</td>\n",
       "      <td>21.39</td>\n",
       "      <td>84.42</td>\n",
       "      <td>521.5</td>\n",
       "      <td>0.13230</td>\n",
       "      <td>0.10400</td>\n",
       "      <td>0.15210</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.07097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>19.270</td>\n",
       "      <td>26.47</td>\n",
       "      <td>127.90</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>0.09401</td>\n",
       "      <td>0.17190</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.075930</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.06261</td>\n",
       "      <td>...</td>\n",
       "      <td>24.15</td>\n",
       "      <td>30.90</td>\n",
       "      <td>161.40</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>0.15090</td>\n",
       "      <td>0.65900</td>\n",
       "      <td>0.60910</td>\n",
       "      <td>0.17850</td>\n",
       "      <td>0.3672</td>\n",
       "      <td>0.11230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>13.900</td>\n",
       "      <td>16.62</td>\n",
       "      <td>88.97</td>\n",
       "      <td>599.4</td>\n",
       "      <td>0.06828</td>\n",
       "      <td>0.05319</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>0.1813</td>\n",
       "      <td>0.05536</td>\n",
       "      <td>...</td>\n",
       "      <td>15.14</td>\n",
       "      <td>21.80</td>\n",
       "      <td>101.20</td>\n",
       "      <td>718.9</td>\n",
       "      <td>0.09384</td>\n",
       "      <td>0.20060</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.06222</td>\n",
       "      <td>0.2679</td>\n",
       "      <td>0.07698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>13.900</td>\n",
       "      <td>19.24</td>\n",
       "      <td>88.73</td>\n",
       "      <td>602.9</td>\n",
       "      <td>0.07991</td>\n",
       "      <td>0.05326</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.05594</td>\n",
       "      <td>...</td>\n",
       "      <td>16.41</td>\n",
       "      <td>26.42</td>\n",
       "      <td>104.40</td>\n",
       "      <td>830.5</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.14150</td>\n",
       "      <td>0.16730</td>\n",
       "      <td>0.08150</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.07603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>12.060</td>\n",
       "      <td>12.74</td>\n",
       "      <td>76.84</td>\n",
       "      <td>448.6</td>\n",
       "      <td>0.09311</td>\n",
       "      <td>0.05241</td>\n",
       "      <td>0.019720</td>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>...</td>\n",
       "      <td>13.14</td>\n",
       "      <td>18.41</td>\n",
       "      <td>84.08</td>\n",
       "      <td>532.8</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.12320</td>\n",
       "      <td>0.08636</td>\n",
       "      <td>0.07025</td>\n",
       "      <td>0.2514</td>\n",
       "      <td>0.07898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>13.430</td>\n",
       "      <td>19.63</td>\n",
       "      <td>85.84</td>\n",
       "      <td>565.4</td>\n",
       "      <td>0.09048</td>\n",
       "      <td>0.06288</td>\n",
       "      <td>0.058580</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.05671</td>\n",
       "      <td>...</td>\n",
       "      <td>17.98</td>\n",
       "      <td>29.87</td>\n",
       "      <td>116.60</td>\n",
       "      <td>993.6</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.15460</td>\n",
       "      <td>0.26440</td>\n",
       "      <td>0.11600</td>\n",
       "      <td>0.2884</td>\n",
       "      <td>0.07371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.090</td>\n",
       "      <td>19.83</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.09342</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.05484</td>\n",
       "      <td>...</td>\n",
       "      <td>30.79</td>\n",
       "      <td>23.87</td>\n",
       "      <td>211.50</td>\n",
       "      <td>2782.0</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.36250</td>\n",
       "      <td>0.37940</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.07277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.93</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2      3       4       5        6        7         8         9   \\\n",
       "226  10.440  15.46   66.62   329.6  0.10530  0.07722  0.006643  0.012160   \n",
       "457  13.210  25.25   84.10   537.9  0.08791  0.05205  0.027720  0.020680   \n",
       "187  11.710  17.19   74.68   420.3  0.09774  0.06141  0.038090  0.032390   \n",
       "33   19.270  26.47  127.90  1162.0  0.09401  0.17190  0.165700  0.075930   \n",
       "477  13.900  16.62   88.97   599.4  0.06828  0.05319  0.022240  0.013390   \n",
       "..      ...    ...     ...     ...      ...      ...       ...       ...   \n",
       "481  13.900  19.24   88.73   602.9  0.07991  0.05326  0.029950  0.020700   \n",
       "158  12.060  12.74   76.84   448.6  0.09311  0.05241  0.019720  0.019630   \n",
       "171  13.430  19.63   85.84   565.4  0.09048  0.06288  0.058580  0.034380   \n",
       "503  23.090  19.83  152.10  1682.0  0.09342  0.12750  0.167600  0.100300   \n",
       "548   9.683  19.34   61.05   285.7  0.08491  0.05030  0.023370  0.009615   \n",
       "\n",
       "         10       11  ...     22     23      24      25       26       27  \\\n",
       "226  0.1788  0.06450  ...  11.52  19.80   73.47   395.4  0.13410  0.11530   \n",
       "457  0.1619  0.05584  ...  14.35  34.23   91.29   632.9  0.12890  0.10630   \n",
       "187  0.1516  0.06095  ...  13.01  21.39   84.42   521.5  0.13230  0.10400   \n",
       "33   0.1853  0.06261  ...  24.15  30.90  161.40  1813.0  0.15090  0.65900   \n",
       "477  0.1813  0.05536  ...  15.14  21.80  101.20   718.9  0.09384  0.20060   \n",
       "..      ...      ...  ...    ...    ...     ...     ...      ...      ...   \n",
       "481  0.1579  0.05594  ...  16.41  26.42  104.40   830.5  0.10640  0.14150   \n",
       "158  0.1590  0.05907  ...  13.14  18.41   84.08   532.8  0.12750  0.12320   \n",
       "171  0.1598  0.05671  ...  17.98  29.87  116.60   993.6  0.14010  0.15460   \n",
       "503  0.1505  0.05484  ...  30.79  23.87  211.50  2782.0  0.11990  0.36250   \n",
       "548  0.1580  0.06235  ...  10.93  25.59   69.10   364.2  0.11990  0.09546   \n",
       "\n",
       "          28       29      30       31  \n",
       "226  0.02639  0.04464  0.2615  0.08269  \n",
       "457  0.13900  0.06005  0.2444  0.06788  \n",
       "187  0.15210  0.10990  0.2572  0.07097  \n",
       "33   0.60910  0.17850  0.3672  0.11230  \n",
       "477  0.13840  0.06222  0.2679  0.07698  \n",
       "..       ...      ...     ...      ...  \n",
       "481  0.16730  0.08150  0.2356  0.07603  \n",
       "158  0.08636  0.07025  0.2514  0.07898  \n",
       "171  0.26440  0.11600  0.2884  0.07371  \n",
       "503  0.37940  0.22640  0.2908  0.07277  \n",
       "548  0.09350  0.03846  0.2552  0.07920  \n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_x, data_y = data.drop([0,1],axis=1), data[1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=233, stratify=data_y)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    357\n",
       "1    212\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72\n",
       "1    42\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Monte-Carlo Simulation: Repeat the following procedures for supervised, un\u0002supervised, and semi-supervised learning M = 30 times, and use randomly se\u0002lected train and test data (make sure you use 20% of both the positve and nega\u0002tive classes as the test set). Then compare the average scores (accuracy, precision,\n",
    "recall, F1-score, and AUC) that you obtain from each algorithm.\n",
    "\n",
    "i. Supervised Learning: Train an L1-penalized SVM to classify the data.\n",
    "Use 5 fold cross validation to choose the penalty parameter. Use normalized\n",
    "data. Report the average accuracy, precision, recall, F1-score, and AUC, for\n",
    "both training and test sets over your M runs. Plot the ROC and report the\n",
    "confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def supervised_learning(x_train, y_train, x_test, y_test, is_plot):\n",
    "    c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "    parameters = {'C':c_list}\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5)\n",
    "    clf.fit(x_train, y_train)\n",
    "    best_C = clf.best_params_['C']\n",
    "    # L1 svm model\n",
    "    svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "    svc.fit(x_train, y_train)\n",
    "    \n",
    "    # accuracy\n",
    "    train_accuracy = svc.score(x_train, y_train)\n",
    "    test_accuracy = svc.score(x_test, y_test)\n",
    "    # confusion matrix\n",
    "    y_train_predict = svc.predict(x_train)\n",
    "    y_test_predict = svc.predict(x_test)\n",
    "    train_confusion_matrix = confusion_matrix(y_train, y_train_predict)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "    # precision\n",
    "    train_precision = precision_score(y_train, y_train_predict)  \n",
    "    test_precision = precision_score(y_test, y_test_predict)  \n",
    "    # recall\n",
    "    train_recall = recall_score(y_train, y_train_predict)  \n",
    "    test_recall = recall_score(y_test, y_test_predict) \n",
    "    # f1\n",
    "    train_f1 = f1_score(y_train, y_train_predict)  \n",
    "    test_f1 = f1_score(y_test, y_test_predict) \n",
    "    # AUC\n",
    "    train_predict_prob = svc.decision_function(x_train)\n",
    "    test_predict_prob = svc.decision_function(x_test)\n",
    "    train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "\n",
    "    if is_plot:\n",
    "        print('Plot the ROC and report the confusion matrix for training and testing in one of the runs:')\n",
    "        train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_predict_prob)\n",
    "        test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_predict_prob)\n",
    "        print('Train Confusion Matrix:\\n', train_confusion_matrix)\n",
    "        print('Test Confusion Matrix:\\n', test_confusion_matrix)\n",
    "        plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "        plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the ROC and report the confusion matrix for training and testing in one of the runs:\n",
      "Train Confusion Matrix:\n",
      " [[282   3]\n",
      " [  6 164]]\n",
      "Test Confusion Matrix:\n",
      " [[70  2]\n",
      " [ 3 39]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spuervised AVG Score:\n",
      " Train Accuracy     0.984689\n",
      "Train Precision    0.987466\n",
      "Train Recall       0.971373\n",
      "Train F1-Score     0.979338\n",
      "Train Auc          0.998648\n",
      "Test Accuracy      0.961988\n",
      "Test Precision     0.963305\n",
      "Test Recall        0.933333\n",
      "Test F1-Score      0.947430\n",
      "Test Auc           0.993011\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Auc']\n",
    "supervised_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "is_plot = True\n",
    "for i in range(30):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=i, stratify=data_y)\n",
    "    normalized_x_train = pd.DataFrame(normalize(x_train))\n",
    "    normalized_x_test = pd.DataFrame(normalize(x_test))\n",
    "    supervised_stat.loc[i] = supervised_learning(normalized_x_train, y_train, normalized_x_test, y_test, is_plot)\n",
    "    is_plot = False\n",
    "\n",
    "print('Spuervised AVG Score:\\n', supervised_stat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ii. Semi-Supervised Learning/ Self-training: select 50% of the positive\n",
    "class along with 50% of the negative class in the training set as labeled data\n",
    "and the rest as unlabelled data. You can select them randomly.\n",
    "\n",
    "A. Train an L1-penalized SVM to classify the labeled data Use normalized\n",
    "data. Choose the penalty parameter using 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C choosed by cross validation is: 100000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=100000.0, dual=False, penalty='l1')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_x_train_labeled, normalized_x_train_unlabeled, y_train_labeled, y_train_unlabeled = train_test_split(normalized_x_train, y_train, test_size=0.5, random_state=233, stratify=y_train)\n",
    "# choose the parameter\n",
    "c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "parameters = {'C':c_list}\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(x_train_labeled, y_train_labeled)\n",
    "best_C = clf.best_params_['C']\n",
    "# build L1 svm model using best C\n",
    "print('best C choosed by cross validation is:', best_C)\n",
    "svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "svc.fit(x_train_labeled, y_train_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "B. Find the unlabeled data point that is the farthest to the decision boundary\n",
    "of the SVM. Let the SVM label it (ignore its true label), and add it to\n",
    "the labeled data, and retrain the SVM. Continue this process until all\n",
    "unlabeled data are used. Test the final SVM on the test data andthe\n",
    "average accuracy, precision, recall, F1-score, and AUC, for both training\n",
    "and test sets over your M runs. Plot the ROC and report the confusion\n",
    "matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def semi_supervised_learning(x_train, x_train_unlabeled, y_train, x_test, y_test, is_plot):\n",
    "    c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "    parameters = {'C':c_list}\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5)\n",
    "    clf.fit(x_train, y_train)\n",
    "    best_C = clf.best_params_['C']\n",
    "    # L1 svm model\n",
    "    svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "    svc.fit(x_train, y_train)\n",
    "    # Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it\n",
    "    while len(x_train_unlabeled) > 0:\n",
    "        # print(len(x_train_unlabeled),len(x_train))\n",
    "        # calculate the distance of each data\n",
    "        distance_decision_boundary = svc.decision_function(x_train_unlabeled)\n",
    "        abs_distance = np.abs(distance_decision_boundary)\n",
    "        # get the index of max distance\n",
    "        indx = abs_distance.argmax()\n",
    "        # get the label for farthest point\n",
    "        label_predict = 0 if distance_decision_boundary[indx] < 0 else 1\n",
    "        # append new label into train set\n",
    "        x_train = np.append(x_train, [x_train_unlabeled.iloc[indx]], axis = 0)\n",
    "        y_train = y_train.append(pd.Series(label_predict))\n",
    "        # delete label from test set\n",
    "        x_train_unlabeled = x_train_unlabeled.drop(x_train_unlabeled.index[indx])\n",
    "        # refit the model\n",
    "        svc.fit(x_train, y_train)\n",
    "    \n",
    "    # accuracy\n",
    "    train_accuracy = svc.score(x_train, y_train)\n",
    "    test_accuracy = svc.score(x_test, y_test)\n",
    "    # confusion matrix\n",
    "    y_train_predict = svc.predict(x_train)\n",
    "    y_test_predict = svc.predict(x_test)\n",
    "    train_confusion_matrix = confusion_matrix(y_train, y_train_predict)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "    # precision\n",
    "    train_precision = precision_score(y_train, y_train_predict)  \n",
    "    test_precision = precision_score(y_test, y_test_predict)  \n",
    "    # recall\n",
    "    train_recall = recall_score(y_train, y_train_predict)  \n",
    "    test_recall = recall_score(y_test, y_test_predict) \n",
    "    # f1\n",
    "    train_f1 = f1_score(y_train, y_train_predict)  \n",
    "    test_f1 = f1_score(y_test, y_test_predict) \n",
    "    # AUC\n",
    "    train_predict_prob = svc.decision_function(x_train)\n",
    "    test_predict_prob = svc.decision_function(x_test)\n",
    "    train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "\n",
    "    if is_plot:\n",
    "        print('Plot the ROC and report the confusion matrix for training and testing in one of the runs:')\n",
    "        train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_predict_prob)\n",
    "        test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_predict_prob)\n",
    "        print('Train Confusion Matrix:\\n', train_confusion_matrix)\n",
    "        print('Test Confusion Matrix:\\n', test_confusion_matrix)\n",
    "        plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "        plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the ROC and report the confusion matrix for training and testing in one of the runs:\n",
      "Train Confusion Matrix:\n",
      " [[292   1]\n",
      " [  2 160]]\n",
      "Test Confusion Matrix:\n",
      " [[71  1]\n",
      " [ 4 38]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAinklEQVR4nO3de3wV5Z3H8c+v4RLkIitEZYE0UREMchECgqwt1GLRUqEVjdZatVWXqqhVtnhZFK3req0VpUuhBXRpC1S0gjdcKoiXKgRIuSk0FYEUtIiVqyjB3/4xk/QQcjmBzAkn832/XueVMzPPzPyeJK/zO888M89j7o6IiMTXl+o7ABERqV9KBCIiMadEICISc0oEIiIxp0QgIhJzjeo7gNpq27at5+Tk1HcYIiJpZenSpR+5e1Zl29IuEeTk5FBYWFjfYYiIpBUz21DVNl0aEhGJOSUCEZGYUyIQEYk5JQIRkZhTIhARibnIEoGZTTGzv5vZqiq2m5mNN7NiM1thZr2iikVERKoWZYtgGjCkmu3nAJ3C19XA/0QYi4iIVCGy5wjcfZGZ5VRTZBjwpAfjYL9lZq3NrJ27b4kqpqj89u2NPFv0t3o7/1l7XmDApwvq7fwikho7W59Cv2sm1/lx6/OBsvbApoTlknDdQYnAzK4maDWQnZ0daVCH8qH+9vqPATg995goQqrRgE8XkLPvPd5vfEK9nF9E0lt9JgKrZF2ls+S4+yRgEkB+fn6kM+k8W/Q31mzZQV67Vknvc3ruMQzr2Z7vnh5tkqrS1KOB0+h6xfP1c34RSWv1mQhKgI4Jyx2AzVGeMJlv+2VJYOa/948yFBGRI0Z93j46B/h+ePdQP2B71P0DZd/2q5PXrhXDeraPMgwRkSNKZC0CM/sdMBBoa2YlwJ1AYwB3nwi8AJwLFAN7gCuiigWC1sDb6z/m9Nxj6vfbfuFUWPlU3R7zg5VwfLe6PaaIxEaUdw1dXMN2B66N6vwVlV0Sqvdv+yufqvsP7uO7QbcRdXc8EYmVtBuG+nCcnntM/XXoJjq+G6hjV0SOEBpiQkQk5pQIRERiLlaXhlKips5gdeyKyBFGLYK6VtYZXBV17IrIEUYtgiioM1hE0ohaBCIiMRfvFoEe7hIRiXmLoKbr+YdCfQAikmbi3SIAXc8XkdiLd4tARESUCERE4i42l4bKp3OcevQ/V6pjV0QkPi2CsukcD6COXRGR+LQIAN5vfIKmcxQRqSA2LQIREamcEoGISMwpEYiIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzEWaCMxsiJmtNbNiM7ulku1Hm9lcM/uzma02syuijEdERA4WWSIwswxgAnAOkAdcbGZ5FYpdC6xx9x7AQOBhM2sSVUwiInKwKFsEfYFid3/P3T8HZgDDKpRxoKWZGdAC+BgojTAmERGpIMpE0B7YlLBcEq5L9DhwCrAZWAnc4O5fVDyQmV1tZoVmVrh169ao4hURiaUoE4FVss4rLH8DKAL+FegJPG5mrQ7ayX2Su+e7e35WVlZdxykiEmtRJoISoGPCcgeCb/6JrgCe9kAxsB7oEmFMIiJSQZSJYAnQycxyww7gi4A5FcpsBM4CMLPjgM7AexHGJCIiFTSK6sDuXmpm1wHzgAxgiruvNrOR4faJwE+BaWa2kuBS0hh3/yiqmERE5GCRJQIAd38BeKHCuokJ7zcDZ0cZg4iIVE9PFouIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEnBKBiEjMJZ0IzKx5lIGIiEj9qDERmNkZZrYGeCdc7mFmv4g8MhERSYlkWgSPEEwgsw3A3f8MfCXKoEREJHWSujTk7psqrNofQSwiIlIPkhmGepOZnQF4OMHM9YSXiUREJP0l0yIYCVxLMPF8CcHcwtdEGJOIiKRQMi2Czu5+SeIKMxsAvBFNSCIikkrJtAgeS3KdiIikoSpbBGbWHzgDyDKzmxI2tSKYg1hERBqA6i4NNQFahGVaJqzfAYyIMigREUmdKhOBu78KvGpm09x9QwpjEhGRFEqms3iPmT0IdAUyy1a6+9cii0pERFImmc7i3wDvArnAXcD7wJIIYxIRkRRKJhG0cfdfA/vc/VV3/wHQL+K4REQkRZK5NLQv/LnFzL4JbAY6RBeSiIikUjKJ4B4zOxq4meD5gVbAjVEGJSIiqVNjInD358K324FBUP5ksYiINADVPVCWAVxIMMbQS+6+ysyGArcBzYDTUhOiiIhEqboWwa+BjsBiYLyZbQD6A7e4+x9SEJuIiKRAdYkgH+ju7l+YWSbwEXCSu3+QmtBERCQVqrt99HN3/wLA3fcC62qbBMxsiJmtNbNiM7ulijIDzazIzFab2au1Ob6IiBy+6loEXcxsRfjegBPDZQPc3btXd+Cwj2ECMJhgHoMlZjbH3dcklGkN/AIY4u4bzezYQ6+KiIgciuoSwSmHeey+QLG7vwdgZjOAYcCahDLfBZ52940A7v73wzyniIjUUnWDzh3uQHPtgcS5jkuA0yuUORlobGYLCUY4fdTdn6x4IDO7GrgaIDs7+zDDEhGRRElNXn+IrJJ1XmG5EdAb+CbwDWCsmZ180E7uk9w9393zs7Ky6j5SEZEYS+bJ4kNVQnD7aZkOBMNTVCzzkbvvBnab2SKgB7AuwrhERCRBUi0CM2tmZp1reewlQCczyzWzJsBFwJwKZZ4FzjSzRmZ2FMGlo3dqeR4RETkMNSYCM/sWUAS8FC73NLOKH+gHcfdS4DpgHsGH+yx3X21mI81sZFjmnfC4KwgeXPuVu686xLqIiMghSObS0DiCO4AWArh7kZnlJHNwd38BeKHCuokVlh8EHkzmeCIiUveSuTRU6u7bI49ERETqRTItglVm9l0gw8w6AdcDb0YbloiIpEoyLYJRBPMVfwb8lmA46hsjjElERFIomRZBZ3e/Hbg96mBERCT1kmkR/MzM3jWzn5pZ18gjEhGRlKoxEbj7IGAgsBWYZGYrzew/ow5MRERSI6kHytz9A3cfD4wkeKbgjiiDEhGR1EnmgbJTzGycma0CHie4Y6hD5JGJiEhKJNNZPBX4HXC2u1ccK0hERNJcjYnA3fulIhAREakfVSYCM5vl7hea2UoOHD46qRnKREQkPVTXIrgh/Dk0FYGIiEj9qLKz2N23hG+vcfcNiS/gmtSEJyIiUUvm9tHBlaw7p64DERGR+lFdH8GPCL75n2BmKxI2tQTeiDowERFJjer6CH4LvAj8N3BLwvqd7v5xpFGJiEjKVJcI3N3fN7NrK24ws2OUDEREGoaaWgRDgaUEt49awjYHTogwLhERSZEqE4G7Dw1/5qYuHBERSbVkxhoaYGbNw/ffM7OfmVl29KGJiEgqJHP76P8Ae8ysB/ATYAPwv5FGJSIiKZPs5PUODAMedfdHCW4hFRGRBiCZ0Ud3mtmtwKXAmWaWATSONiwREUmVZFoEBQQT1//A3T8A2gMPRhqViIikTDJTVX4A/AY42syGAnvd/cnIIxMRkZRI5q6hC4HFwAXAhcDbZjYi6sBERCQ1kukjuB3o4+5/BzCzLGA+8FSUgYmISGok00fwpbIkENqW5H4iIpIGkmkRvGRm8wjmLYag8/iF6EISEZFUSmbO4v8ws+8A/0Yw3tAkd38m8shERCQlqpuPoBPwEHAisBIY7e5/S1VgIiKSGtVd658CPAecTzAC6WO1PbiZDTGztWZWbGa3VFOuj5nt191IIiKpV92loZbuPjl8v9bMltXmwOETyBMIprosAZaY2Rx3X1NJufuBebU5voiI1I3qEkGmmZ3GP+chaJa47O41JYa+QLG7vwdgZjMIxitaU6HcKGA20KeWsYuISB2oLhFsAX6WsPxBwrIDX6vh2O2BTQnLJcDpiQXMrD3w7fBYVSYCM7sauBogO1sjYIuI1KXqJqYZdJjHtkrWeYXlnwNj3H2/WWXFy2OZBEwCyM/Pr3gMERE5DMk8R3CoSoCOCcsdgM0VyuQDM8Ik0BY418xK3f0PEcYlIiIJokwES4BOZpYL/A24CPhuYoHEaTDNbBrwnJKAiEhqRZYI3L3UzK4juBsoA5ji7qvNbGS4fWJU5xYRkeTVmAgsuG5zCXCCu98dzld8vLsvrmlfd3+BCsNRVJUA3P3ypCIWEZE6lczgcb8A+gMXh8s7CZ4PEBGRBiCZS0Onu3svM1sO4O7/MLMmEcclIiIpkkyLYF/49K9D+XwEX0QalYiIpEwyiWA88AxwrJn9F/A6cG+kUYmISMokMwz1b8xsKXAWwUNiw939ncgjExGRlEjmrqFsYA8wN3Gdu2+MMjAREUmNZDqLnyfoHzAgE8gF1gJdI4xLRERSJJlLQ90Sl82sF/DvkUUkIiIpVetJ6MPhpzVktIhIA5FMH8FNCYtfAnoBWyOLSEREUiqZPoKWCe9LCfoMZkcTjoiIpFq1iSB8kKyFu/9HiuIREZEUq7KPwMwauft+gktBIiLSQFXXIlhMkASKzGwO8Htgd9lGd3864thERCQFkukjOAbYRjCvcNnzBA4oEYiINADVJYJjwzuGVvHPBFBG8waLiDQQ1SWCDKAFyU1CLyIiaaq6RLDF3e9OWSQiIlIvqnuyuLKWgIiINDDVJYKzUhaFiIjUmyoTgbt/nMpARESkftR60DkREWlYlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOYiTQRmNsTM1ppZsZndUsn2S8xsRfh608x6RBmPiIgcLLJEEM53PAE4B8gDLjazvArF1gNfdffuwE+BSVHFIyIilYuyRdAXKHb399z9c2AGMCyxgLu/6e7/CBffAjpEGI+IiFQiykTQHtiUsFwSrqvKD4EXK9tgZlebWaGZFW7durUOQxQRkSgTQdIzm5nZIIJEMKay7e4+yd3z3T0/KyurDkMUEZFkJq8/VCVAx4TlDsDmioXMrDvwK+Acd98WYTwiIlKJKFsES4BOZpZrZk2Ai4A5iQXMLBt4GrjU3ddFGIuIiFQhshaBu5ea2XXAPCADmOLuq81sZLh9InAH0Ab4hZkBlLp7flQxiYjIwaK8NIS7vwC8UGHdxIT3VwJXRhmDiIhUT08Wi4jEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzEV6+6iIpId9+/ZRUlLC3r176zsUOUyZmZl06NCBxo0bJ72PEoGIUFJSQsuWLcnJySF8uFPSkLuzbds2SkpKyM3NTXo/XRoSEfbu3UubNm2UBNKcmdGmTZtat+yUCEQEQEmggTiUv6MSgYhIzCkRiEi927ZtGz179qRnz54cf/zxtG/fvnz5888/r3bfwsJCrr/++lqdLycnh27dutG9e3e++tWvsmHDhvJtJSUlDBs2jE6dOnHiiSdyww03HBDD4sWL+cpXvkLnzp3p0qULV155JXv27KldhY8wSgQiUu/atGlDUVERRUVFjBw5kh//+Mfly02aNKG0tLTKffPz8xk/fnytz7lgwQJWrFjBwIEDueeee4Cgs/U73/kOw4cP5y9/+Qvr1q1j165d3H777QB8+OGHXHDBBdx///2sXbuWd955hyFDhrBz585Dq3glqqtrVHTXkIgc4K65q1mzeUedHjPvX1tx57e61mqfyy+/nGOOOYbly5fTq1cvCgoKuPHGG/n0009p1qwZU6dOpXPnzixcuJCHHnqI5557jnHjxrFx40bee+89Nm7cyI033lhja6F///7lieSVV14hMzOTK664AoCMjAweeeQRcnNzueuuu5gwYQKXXXYZ/fv3B4Lr8SNGjDjomPv372fMmDHMmzcPM+Oqq65i1KhR5OTkUFhYSNu2bSksLGT06NEsXLiQcePGsXnzZt5//33atm3LX//6V6ZMmULXrsHvbODAgTz88MN06dKFUaNGsXLlSkpLSxk3bhzDhg076Py1pUQgIkesdevWMX/+fDIyMtixYweLFi2iUaNGzJ8/n9tuu43Zs2cftM+7777LggUL2LlzJ507d+ZHP/pRtffUv/TSSwwfPhyA1atX07t37wO2t2rViuzsbIqLi1m1ahWXXXZZjXFPmjSJ9evXs3z5cho1asTHH39c4z5Lly7l9ddfp1mzZjzyyCPMmjWLu+66iy1btrB582Z69+7Nbbfdxte+9jWmTJnCJ598Qt++ffn6179O8+bNazx+dZQIROQAtf3mHqULLriAjIwMALZv385ll13GX/7yF8yMffv2VbrPN7/5TZo2bUrTpk059thj+fDDD+nQocNB5QYNGsSHH37Isccee8ClocruuqlqfVXmz5/PyJEjadQo+Ig95phjatznvPPOo1mzZgBceOGFDB48mLvuuotZs2ZxwQUXAPDyyy8zZ84cHnroISC47Xfjxo2ccsopScdWGfURiMgRK/Gb7tixYxk0aBCrVq1i7ty5Vd4r37Rp0/L3GRkZVV5zX7BgARs2bKBr167ccccdAHTt2pXCwsIDyu3YsYNNmzZx4okn0rVrV5YuXVpj3FUljkaNGvHFF18AHBR/Yl3bt29PmzZtWLFiBTNnzuSiiy4qP+7s2bPL+0/qIgmAEoGIpInt27fTvn17AKZNm1Ynx2zWrBk///nPefLJJ/n4448566yz2LNnD08++SQQXOu/+eabufzyyznqqKO47rrreOKJJ3j77bfLjzF9+nQ++OCDA4579tlnM3HixPIkVHZpKCcnpzyRVHZZK9FFF13EAw88wPbt2+nWrRsA3/jGN3jsscdwdwCWL19eB78FJQIRSRM/+clPuPXWWxkwYAD79++vs+O2a9eOiy++mAkTJmBmPPPMM/z+97+nU6dOnHzyyWRmZnLvvfcCcNxxxzFjxgxGjx5N586dOeWUU3jttddo1arVAce88soryc7Opnv37vTo0YPf/va3ANx5553ccMMNnHnmmeWXvKoyYsQIZsyYwYUXXli+buzYsezbt4/u3btz6qmnMnbs2Dr5HVhZZkkX+fn5XrHplozV9/4bAF1ve72uQxJJe++8806dXGKQI0Nlf08zW+ru+ZWVV4tARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTolAROrd4QxDDbBw4ULefPPNSrdNmzaNrKwsevbsSZcuXXjkkUcO2D5p0iS6dOlCly5d6Nu3L6+//s9bzPft28ctt9xCp06dOPXUU+nbty8vvvji4VX2CKSxhkSk3pUNQw0wbtw4WrRowejRo5Pef+HChbRo0YIzzjij0u0FBQU8/vjjbNu2jc6dOzNixAg6duzIc889xy9/+Utef/112rZty7Jlyxg+fDiLFy/m+OOPZ+zYsWzZsoVVq1bRtGlTPvzwQ1599dW6qHK5/fv31/hwWdSUCETkQC/eAh+srNtjHt8NzrmvVrssXbqUm266iV27dtG2bVumTZtGu3btGD9+PBMnTqRRo0bk5eVx3333MXHiRDIyMpg+fTqPPfYYZ555ZqXHbNOmDSeddBJbtmyhY8eO3H///Tz44IO0bdsWgF69enHZZZcxYcIEbr31ViZPnsz69evLxy867rjjDnjSt8ySJUu44YYb2L17N02bNuWPf/wjs2fPprCwkMcffxyAoUOHMnr0aAYOHEiLFi246aabmDdvHkOHDmXlypXMmjULCJLaww8/zNy5c3n55Ze58847+eyzzzjxxBOZOnUqLVq0qNXvMRlKBCJyxHF3Ro0axbPPPktWVhYzZ87k9ttvZ8qUKdx3333lH86ffPIJrVu3ZuTIkUm1IjZu3MjevXvp3r07UPmw0/n5+TzxxBMUFxeTnZ190PARFX3++ecUFBQwc+ZM+vTpw44dO8pHEa3K7t27OfXUU7n77rspLS3lhBNOYPfu3TRv3pyZM2dSUFDARx99xD333MP8+fNp3rw5999/Pz/72c/KB8irS0oEInKgWn5zj8Jnn33GqlWrGDx4MBBcPmnXrh0A3bt355JLLmH48OHl8wjUZObMmSxYsIC1a9cyefJkMjMzqyxb2yGn165dS7t27ejTpw9AjYkDglFRzz//fCAYkXTIkCHMnTuXESNG8Pzzz/PAAw/w6quvsmbNGgYMGAAECadsQpy6FmlnsZkNMbO1ZlZsZrdUst3MbHy4fYWZ9YoyHhFJD+5O165dy4dbXrlyJS+//DIAzz//PNdeey1Lly6ld+/eSU3tWFBQwOrVq3nttde4+eaby0cLzcvLO2hY6WXLlpGXl8dJJ53Exo0ba5yGMpkhp+HAYaczMzMP6BcoKChg1qxZvPLKK/Tp04eWLVvi7gwePLj8d7BmzRp+/etf11jXQxFZIjCzDGACcA6QB1xsZnkVip0DdApfVwP/E1U8IpI+mjZtytatW/nTn/4EBHfvrF69mi+++IJNmzYxaNAgHnjgAT755BN27dpFy5Ytk5o3uH///lx66aU8+uijQDCi6ZgxY9i2bRsARUVFTJs2jWuuuYajjjqKH/7wh1x//fXldy5t2bKF6dOnH3DMLl26sHnzZpYsWQLAzp07KS0tJScnh6KiovKYFy9eXGVcAwcOZNmyZUyePJmCggIA+vXrxxtvvEFxcTEAe/bsYd26dbX5NSYtyhZBX6DY3d9z98+BGUDFyTWHAU964C2gtZm1izAmEUkDX/rSl3jqqacYM2YMPXr0oGfPnrz55pvs37+f733ve3Tr1o3TTjuNH//4x7Ru3ZpvfetbPPPMM/Ts2ZPXXnut2mOPGTOGqVOnsnPnTs477zx+8IMfcMYZZ9ClSxeuuuoqpk+fXn4Z6p577iErK4u8vDxOPfVUhg8fTlZW1gHHa9KkCTNnzmTUqFH06NGDwYMHs3fvXgYMGEBubi7dunVj9OjR9OpV9QWPjIwMhg4dyosvvsjQoUMByMrKYtq0aVx88cV0796dfv368e677x7mb7ZykQ1DbWYjgCHufmW4fClwurtfl1DmOeA+d389XP4jMMbdCysc62qCFgPZ2dm9N2zYUOt43vrFVQD0u2byIdVHpCHTMNQNS22HoY6ys7iy3paKWSeZMrj7JGASBPMRHEowSgAiIpWL8tJQCdAxYbkDsPkQyoiISISiTARLgE5mlmtmTYCLgDkVyswBvh/ePdQP2O7uWyKMSUSqkG6zFUrlDuXvGNmlIXcvNbPrgHlABjDF3Veb2chw+0TgBeBcoBjYA1wRVTwiUrXMzEy2bdtGmzZtanUPvRxZ3J1t27ZV+5xEZWIzZ7GIVG3fvn2UlJQccK+7pKfMzEw6dOhA48aND1hfX53FIpImGjduTG5ubn2HIfVEw1CLiMScEoGISMwpEYiIxFzadRab2Vag9o8WB9oCH9VhOOlAdY4H1TkeDqfOX3b3rMo2pF0iOBxmVlhVr3lDpTrHg+ocD1HVWZeGRERiTolARCTm4pYIJtV3APVAdY4H1TkeIqlzrPoIRETkYHFrEYiISAVKBCIiMdcgE4GZDTGztWZWbGa3VLLdzGx8uH2FmVU9h1yaSKLOl4R1XWFmb5pZj/qIsy7VVOeEcn3MbH84a15aS6bOZjbQzIrMbLWZvZrqGOtaEv/bR5vZXDP7c1jntB7F2MymmNnfzWxVFdvr/vPL3RvUi2DI678CJwBNgD8DeRXKnAu8SDBDWj/g7fqOOwV1PgP4l/D9OXGoc0K5VwiGPB9R33Gn4O/cGlgDZIfLx9Z33Cmo823A/eH7LOBjoEl9x34Ydf4K0AtYVcX2Ov/8aogtgr5Asbu/5+6fAzOAYRXKDAOe9MBbQGsza5fqQOtQjXV29zfd/R/h4lsEs8Gls2T+zgCjgNnA31MZXESSqfN3gafdfSOAu6d7vZOpswMtLZhIoQVBIihNbZh1x90XEdShKnX++dUQE0F7YFPCckm4rrZl0klt6/NDgm8U6azGOptZe+DbwMQUxhWlZP7OJwP/YmYLzWypmX0/ZdFFI5k6Pw6cQjDN7UrgBnf/IjXh1Ys6//xqiPMRVDa9UsV7ZJMpk06Sro+ZDSJIBP8WaUTRS6bOPwfGuPv+BjLrVjJ1bgT0Bs4CmgF/MrO33H1d1MFFJJk6fwMoAr4GnAj8n5m95u47Io6tvtT551dDTAQlQMeE5Q4E3xRqWyadJFUfM+sO/Ao4x923pSi2qCRT53xgRpgE2gLnmlmpu/8hJRHWvWT/tz9y993AbjNbBPQA0jURJFPnK4D7PLiAXmxm64EuwOLUhJhydf751RAvDS0BOplZrpk1AS4C5lQoMwf4ftj73g/Y7u5bUh1oHaqxzmaWDTwNXJrG3w4T1Vhnd8919xx3zwGeAq5J4yQAyf1vPwucaWaNzOwo4HTgnRTHWZeSqfNGghYQZnYc0Bl4L6VRpladf341uBaBu5ea2XXAPII7Dqa4+2ozGxlun0hwB8m5QDGwh+AbRdpKss53AG2AX4TfkEs9jUduTLLODUoydXb3d8zsJWAF8AXwK3ev9DbEdJDk3/mnwDQzW0lw2WSMu6ft8NRm9jtgINDWzEqAO4HGEN3nl4aYEBGJuYZ4aUhERGpBiUBEJOaUCEREYk6JQEQk5pQIRERiTolAjkjhaKFFCa+casruqoPzTTOz9eG5lplZ/0M4xq/MLC98f1uFbW8ebozhccp+L6vCETdb11C+p5mdWxfnloZLt4/KEcnMdrl7i7ouW80xpgHPuftTZnY28JC7dz+M4x12TDUd18yeANa5+39VU/5yIN/dr6vrWKThUItA0oKZtTCzP4bf1lea2UEjjZpZOzNblPCN+cxw/dlm9qdw39+bWU0f0IuAk8J9bwqPtcrMbgzXNTez58Px71eZWUG4fqGZ5ZvZfUCzMI7fhNt2hT9nJn5DD1si55tZhpk9aGZLLBhj/t+T+LX8iXCwMTPra8E8E8vDn53DJ3HvBgrCWArC2KeE51le2e9RYqi+x97WS6/KXsB+goHEioBnCJ6CbxVua0vwVGVZi3ZX+PNm4PbwfQbQMiy7CGgerh8D3FHJ+aYRzlcAXAC8TTB420qgOcHwxquB04DzgckJ+x4d/lxI8O27PKaEMmUxfht4InzfhGAUyWbA1cB/huubAoVAbiVx7kqo3++BIeFyK6BR+P7rwOzw/eXA4wn73wt8L3zfmmAMoub1/ffWq35fDW6ICWkwPnX3nmULZtYYuNfMvkIwdEJ74Djgg4R9lgBTwrJ/cPciM/sqkAe8EQ6t0YTgm3RlHjSz/wS2EozQehbwjAcDuGFmTwNnAi8BD5nZ/QSXk16rRb1eBMabWVNgCLDI3T8NL0d1t3/OonY00AlYX2H/ZmZWBOQAS4H/Syj/hJl1IhiJsnEV5z8bOM/MRofLmUA26T0ekRwmJQJJF5cQzD7V2933mdn7BB9i5dx9UZgovgn8r5k9CPwD+D93vziJc/yHuz9VtmBmX6+skLuvM7PeBOO9/LeZvezudydTCXffa2YLCYZOLgB+V3Y6YJS7z6vhEJ+6e08zOxp4DrgWGE8w3s4Cd/922LG+sIr9DTjf3dcmE6/Eg/oIJF0cDfw9TAKDgC9XLGBmXw7LTAZ+TTDd31vAADMru+Z/lJmdnOQ5FwHDw32aE1zWec3M/hXY4+7TgYfC81S0L2yZVGYGwUBhZxIMpkb480dl+5jZyeE5K+Xu24HrgdHhPkcDfws3X55QdCfBJbIy84BRFjaPzOy0qs4h8aFEIOniN0C+mRUStA7eraTMQKDIzJYTXMd/1N23Enww/s7MVhAkhi7JnNDdlxH0HSwm6DP4lbsvB7oBi8NLNLcD91Sy+yRgRVlncQUvE8xLO9+D6RchmCdiDbDMgknLf0kNLfYwlj8TDM38AEHr5A2C/oMyC4C8ss5igpZD4zC2VeGyxJxuHxURiTm1CEREYk6JQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYu7/AXCTrMLo4qqWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:03<00:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Spuervised AVG Score:\n",
      " Train Accuracy     0.995092\n",
      "Train Precision    0.996623\n",
      "Train Recall       0.990079\n",
      "Train F1-Score     0.993327\n",
      "Train Auc          0.999586\n",
      "Test Accuracy      0.951462\n",
      "Test Precision     0.944718\n",
      "Test Recall        0.923810\n",
      "Test F1-Score      0.933296\n",
      "Test Auc           0.988668\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Auc']\n",
    "semi_supervised_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "is_plot = True\n",
    "for i in tqdm(range(30)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=i, stratify=data_y)\n",
    "    normalized_x_train = pd.DataFrame(normalize(x_train))\n",
    "    normalized_x_test = pd.DataFrame(normalize(x_test))\n",
    "    normalized_x_train_labeled, normalized_x_train_unlabeled, y_train_labeled, y_train_unlabeled = train_test_split(normalized_x_train, y_train, test_size=0.5, random_state=i, stratify=y_train)\n",
    "    semi_supervised_stat.loc[i] = semi_supervised_learning(normalized_x_train_labeled, normalized_x_train_unlabeled, y_train_labeled, normalized_x_test, y_test, is_plot)\n",
    "    is_plot = False\n",
    "print('Semi-Spuervised AVG Score:\\n', semi_supervised_stat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "iii. Unsupervised Learning: Run k-means algorithm on the whole training\n",
    "set. Ignore the labels of the data, and assume k = 2.\n",
    "\n",
    "A. Run the k-means algorithm multiple times. Make sure that you initialize\n",
    "the algoritm randomly. How do you make sure that the algorithm was\n",
    "not trapped in a local minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To avoid local minimum, we can set the n_init large to run k-means multiple times and then choosing the best out of that (the run that gives the lowest error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "for i in range(30):\n",
    "    clusterer = KMeans(n_clusters=2, n_init = 100, random_state=i).fit(normalized_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Compute the centers of the two clusters and find the closest 30 data\n",
    "points to each center. Read the true labels of those 30 data points and\n",
    "take a majority poll within them. The majority poll becomes the label\n",
    "predicted by k-means for the members of each cluster. Then compare the\n",
    "labels provided by k-means with the true labels of the training data and\n",
    "report the average accuracy, precision, recall, F1-score, and AUC over M\n",
    "runs, and ROC and the confusion matrix for one of the runs.1\n",
    "\n",
    "C. Classify test data based on their proximity to the centers of the clusters.\n",
    "Report the average accuracy, precision, recall, F1-score, and AUC over\n",
    "M runs, and ROC and the confusion matrix for one of the runs for the\n",
    "test data.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "def unsupervised_learning(x_train, y_train, x_test, y_test, is_plot):\n",
    "    clusterer = KMeans(n_clusters=2, n_init = 100).fit(x_train)\n",
    "    labels = clusterer.labels_\n",
    "    l_train, l_test = len(x_train), len(x_test)\n",
    "\n",
    "    train_dis = euclidean_distances(x_train, clusterer.cluster_centers_)\n",
    "    test_dis = euclidean_distances(x_test, clusterer.cluster_centers_)\n",
    "\n",
    "    # Read the true labels of those 30 data points and take a majority poll within them\n",
    "    y_train = np.array(y_train).reshape((l_train, 1))\n",
    "    dis_and_label = np.concatenate((train_dis, y_train), axis=1)\n",
    "    dis_and_label = pd.DataFrame(dis_and_label, columns=['dis_to_center1', 'dis_to_center2', 'label'])\n",
    "    dis_and_label_1 =dis_and_label.sort_values('dis_to_center1').iloc[:30]\n",
    "    dis_and_label_2 =dis_and_label.sort_values('dis_to_center2').iloc[:30]\n",
    "    label1, label2 = dis_and_label_1['label'].value_counts().index[0], dis_and_label_2['label'].value_counts().index[0]\n",
    "    # print(label1, label2)\n",
    "\n",
    "    train_predict = []\n",
    "    train_predict_prob = []\n",
    "    for i in range(l_train):\n",
    "        pred_label = label1 if labels[i]==0 else label2\n",
    "        train_predict_prob.append([train_dis[i][0],train_dis[i][1]])\n",
    "        train_predict.append(pred_label)\n",
    "    train_predict_prob = softmax(train_predict_prob, axis=1).astype(np.float64)\n",
    "    train_predict_prob = train_predict_prob[:, 0]\n",
    "    \n",
    "    test_predict = []\n",
    "    test_predict_prob = []\n",
    "    for i in range(l_test):\n",
    "        pred_label = label1 if test_dis[i][0]<test_dis[i][1] else label2\n",
    "        test_predict_prob.append([test_dis[i][0],test_dis[i][1]])\n",
    "        test_predict.append(pred_label)\n",
    "    test_predict_prob = softmax(test_predict_prob, axis=1).astype(np.float64)\n",
    "    test_predict_prob = test_predict_prob[:, 0]\n",
    "    \n",
    "    # accuracy\n",
    "    train_accuracy = accuracy_score(train_predict, y_train)\n",
    "    test_accuracy = accuracy_score(test_predict, y_test)\n",
    "    # confusion matrix\n",
    "    train_confusion_matrix = confusion_matrix(y_train, train_predict)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, test_predict)\n",
    "    # precision\n",
    "    train_precision = precision_score(y_train, train_predict)  \n",
    "    test_precision = precision_score(y_test, test_predict)  \n",
    "    # recall\n",
    "    train_recall = recall_score(y_train, train_predict)  \n",
    "    test_recall = recall_score(y_test, test_predict) \n",
    "    # f1\n",
    "    train_f1 = f1_score(y_train, train_predict)  \n",
    "    test_f1 = f1_score(y_test, test_predict) \n",
    "    # AUC\n",
    "    train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "\n",
    "    if is_plot:\n",
    "        print('Plot the ROC and report the confusion matrix for training and testing in one of the runs:')\n",
    "        train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_predict_prob)\n",
    "        test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_predict_prob)\n",
    "        print('Train Confusion Matrix:\\n', train_confusion_matrix)\n",
    "        print('Test Confusion Matrix:\\n', test_confusion_matrix)\n",
    "        plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "        plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the ROC and report the confusion matrix for training and testing in one of the runs:\n",
      "Train Confusion Matrix:\n",
      " [[280   5]\n",
      " [ 51 119]]\n",
      "Test Confusion Matrix:\n",
      " [[71  1]\n",
      " [10 32]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:05<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspuervised Kmeans AVG Score:\n",
      " Train Accuracy     0.888498\n",
      "Train Precision    0.959018\n",
      "Train Recall       0.733137\n",
      "Train F1-Score     0.830604\n",
      "Train Auc          0.680442\n",
      "Test Accuracy      0.880117\n",
      "Test Precision     0.964111\n",
      "Test Recall        0.700794\n",
      "Test F1-Score      0.810250\n",
      "Test Auc           0.680776\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Auc']\n",
    "unsupervised_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "is_plot = True\n",
    "for i in tqdm(range(30)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=i, stratify=data_y)\n",
    "    normalized_x_train = pd.DataFrame(normalize(x_train))\n",
    "    normalized_x_test = pd.DataFrame(normalize(x_test))\n",
    "    unsupervised_stat.loc[i] = unsupervised_learning(normalized_x_train, y_train, normalized_x_test, y_test, is_plot)\n",
    "    is_plot = False\n",
    "print('Unspuervised Kmeans AVG Score:\\n', unsupervised_stat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clus\u0002tering based on kernels.3 Research what spectral clustering is. Use RBF\n",
    "kernel with gamma=1 or find a gamma for which the two clutsres have the\n",
    "same balance as the one in original data set (if the positive class has p and the\n",
    "negative class has n samples, the two clusters must have p and n members).\n",
    "Do not label data based on their proximity to cluster center, because spectral\n",
    "clustering may give you non-convex clusters . Instead, use fit t predict\n",
    "method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def unsupervised_learning_spectral(x_train, y_train, x_test, y_test, is_plot):\n",
    "    labels = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit_predict(x_train)\n",
    "    #  consider labeling based on the entire points in each cluster\n",
    "    centers =[]\n",
    "    for i in [0,1]:\n",
    "        centers.append(np.mean(x_train[labels==i], axis=0).to_list())\n",
    "\n",
    "    l_train, l_test = len(x_train), len(x_test)\n",
    "\n",
    "    train_dis = euclidean_distances(x_train, centers)\n",
    "    test_dis = euclidean_distances(x_test, centers)\n",
    "    # Read the true labels of those 30 data points and take a majority poll within them\n",
    "    y_train = np.array(y_train).reshape((l_train, 1))\n",
    "    dis_and_label = np.concatenate((train_dis, y_train), axis=1)\n",
    "    dis_and_label = pd.DataFrame(dis_and_label, columns=['dis_to_center1', 'dis_to_center2', 'label'])\n",
    "    dis_and_label_1 =dis_and_label.sort_values('dis_to_center1').iloc[:30]\n",
    "    dis_and_label_2 =dis_and_label.sort_values('dis_to_center2').iloc[:30]\n",
    "    label1, label2 = dis_and_label_1['label'].value_counts().index[0], dis_and_label_2['label'].value_counts().index[0]\n",
    "\n",
    "    train_predict = []\n",
    "    train_predict_prob = []\n",
    "    for i in range(l_train):\n",
    "        pred_label = label1 if labels[i]==0 else label2\n",
    "        train_predict_prob.append([train_dis[i][0],train_dis[i][1]])\n",
    "        train_predict.append(pred_label)\n",
    "    train_predict_prob = softmax(train_predict_prob, axis=1).astype(np.float64)\n",
    "    train_predict_prob = train_predict_prob[:, 0]\n",
    "    \n",
    "    test_predict = []\n",
    "    test_predict_prob = []\n",
    "    for i in range(l_test):\n",
    "        pred_label = label1 if test_dis[i][0]<test_dis[i][1] else label2\n",
    "        test_predict_prob.append([test_dis[i][0],test_dis[i][1]])\n",
    "        test_predict.append(pred_label)\n",
    "    test_predict_prob = softmax(test_predict_prob, axis=1).astype(np.float64)\n",
    "    test_predict_prob = test_predict_prob[:, 0]\n",
    "    \n",
    "    # accuracy\n",
    "    train_accuracy = accuracy_score(train_predict, y_train)\n",
    "    test_accuracy = accuracy_score(test_predict, y_test)\n",
    "    # confusion matrix\n",
    "    train_confusion_matrix = confusion_matrix(y_train, train_predict)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, test_predict)\n",
    "    # precision\n",
    "    train_precision = precision_score(y_train, train_predict)  \n",
    "    test_precision = precision_score(y_test, test_predict)  \n",
    "    # recall\n",
    "    train_recall = recall_score(y_train, train_predict)  \n",
    "    test_recall = recall_score(y_test, test_predict) \n",
    "    # f1\n",
    "    train_f1 = f1_score(y_train, train_predict)  \n",
    "    test_f1 = f1_score(y_test, test_predict) \n",
    "    # AUC\n",
    "    train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "\n",
    "    if is_plot:\n",
    "        print('Plot the ROC and report the confusion matrix for training and testing in one of the runs:')\n",
    "        train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_predict_prob)\n",
    "        test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_predict_prob)\n",
    "        print('Train Confusion Matrix:\\n', train_confusion_matrix)\n",
    "        print('Test Confusion Matrix:\\n', test_confusion_matrix)\n",
    "        plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "        plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the ROC and report the confusion matrix for training and testing in one of the runs:\n",
      "Train Confusion Matrix:\n",
      " [[280   5]\n",
      " [ 52 118]]\n",
      "Test Confusion Matrix:\n",
      " [[71  1]\n",
      " [10 32]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 22.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspuervised Spectral clustering AVG Score:\n",
      " Train Accuracy     0.883297\n",
      "Train Precision    0.960396\n",
      "Train Recall       0.717255\n",
      "Train F1-Score     0.821060\n",
      "Train Auc          0.741135\n",
      "Test Accuracy      0.878947\n",
      "Test Precision     0.964995\n",
      "Test Recall        0.696825\n",
      "Test F1-Score      0.808043\n",
      "Test Auc           0.731459\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Auc']\n",
    "unsupervised_stat2 = pd.DataFrame(columns=cols, index=range(30))\n",
    "is_plot = True\n",
    "for i in tqdm(range(30)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=i, stratify=data_y)\n",
    "    normalized_x_train = pd.DataFrame(normalize(x_train))\n",
    "    normalized_x_test = pd.DataFrame(normalize(x_test))\n",
    "    unsupervised_stat2.loc[i] = unsupervised_learning_spectral(normalized_x_train, y_train, normalized_x_test, y_test, is_plot)\n",
    "    is_plot = False\n",
    "print('Unspuervised Spectral clustering AVG Score:\\n', unsupervised_stat2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "v. One can expect that supervised learning on the full data set works better than\n",
    "semi-supervised learning with half of the data set labeled.One can expect that\n",
    "unsupervised learning underperforms in such situations. Compare the results\n",
    "you obtained by those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supervised</th>\n",
       "      <th>semi_supervised</th>\n",
       "      <th>k_means_cluster_unsupervised</th>\n",
       "      <th>spectral_cluster_unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.984689</td>\n",
       "      <td>0.995092</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.883297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.987466</td>\n",
       "      <td>0.996623</td>\n",
       "      <td>0.959018</td>\n",
       "      <td>0.960396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.971373</td>\n",
       "      <td>0.990079</td>\n",
       "      <td>0.733137</td>\n",
       "      <td>0.717255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1-Score</th>\n",
       "      <td>0.979338</td>\n",
       "      <td>0.993327</td>\n",
       "      <td>0.830604</td>\n",
       "      <td>0.821060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Auc</th>\n",
       "      <td>0.998648</td>\n",
       "      <td>0.999586</td>\n",
       "      <td>0.680442</td>\n",
       "      <td>0.741135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Accuracy</th>\n",
       "      <td>0.961988</td>\n",
       "      <td>0.951462</td>\n",
       "      <td>0.880117</td>\n",
       "      <td>0.878947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Precision</th>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.944718</td>\n",
       "      <td>0.964111</td>\n",
       "      <td>0.964995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Recall</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.696825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test F1-Score</th>\n",
       "      <td>0.947430</td>\n",
       "      <td>0.933296</td>\n",
       "      <td>0.810250</td>\n",
       "      <td>0.808043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Auc</th>\n",
       "      <td>0.993011</td>\n",
       "      <td>0.988668</td>\n",
       "      <td>0.680776</td>\n",
       "      <td>0.731459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 supervised  semi_supervised  k_means_cluster_unsupervised  \\\n",
       "Train Accuracy     0.984689         0.995092                      0.888498   \n",
       "Train Precision    0.987466         0.996623                      0.959018   \n",
       "Train Recall       0.971373         0.990079                      0.733137   \n",
       "Train F1-Score     0.979338         0.993327                      0.830604   \n",
       "Train Auc          0.998648         0.999586                      0.680442   \n",
       "Test Accuracy      0.961988         0.951462                      0.880117   \n",
       "Test Precision     0.963305         0.944718                      0.964111   \n",
       "Test Recall        0.933333         0.923810                      0.700794   \n",
       "Test F1-Score      0.947430         0.933296                      0.810250   \n",
       "Test Auc           0.993011         0.988668                      0.680776   \n",
       "\n",
       "                 spectral_cluster_unsupervised  \n",
       "Train Accuracy                        0.883297  \n",
       "Train Precision                       0.960396  \n",
       "Train Recall                          0.717255  \n",
       "Train F1-Score                        0.821060  \n",
       "Train Auc                             0.741135  \n",
       "Test Accuracy                         0.878947  \n",
       "Test Precision                        0.964995  \n",
       "Test Recall                           0.696825  \n",
       "Test F1-Score                         0.808043  \n",
       "Test Auc                              0.731459  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'supervised':supervised_stat.mean(), \n",
    "                        'semi_supervised':semi_supervised_stat.mean(),\n",
    "                        'k_means_cluster_unsupervised':unsupervised_stat.mean(), \n",
    "                        'spectral_cluster_unsupervised':unsupervised_stat2.mean()})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# according to the results,  supervised learning does performs better on test set than semi-supervised learning, unsupervised learning underperforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Active Learning Using Support Vector Machines\n",
    "\n",
    "(a) Download the banknote authentication Data Set from: https://archive.ics.\n",
    "uci.edu/ml/datasets/banknote+authentication. Choose 472 data points ran\u0002domly as the test set, and the remaining 900 points as the training set. This is a\n",
    "binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('../data/Homework 6 Data/data_banknote_authentication.txt', header=None)\n",
    "dataX = data2.iloc[:,0:4]\n",
    "dataY = data2[4]\n",
    "trainX, testX, trainY, testY = train_test_split(dataX, dataY, test_size=472/(900+472))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>2.46730</td>\n",
       "      <td>1.39260</td>\n",
       "      <td>1.71250</td>\n",
       "      <td>0.41421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2.56500</td>\n",
       "      <td>8.63300</td>\n",
       "      <td>-2.99410</td>\n",
       "      <td>-1.30820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.24811</td>\n",
       "      <td>-0.17797</td>\n",
       "      <td>4.90680</td>\n",
       "      <td>0.15429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>-2.59120</td>\n",
       "      <td>-0.10554</td>\n",
       "      <td>1.27980</td>\n",
       "      <td>1.04140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2.67990</td>\n",
       "      <td>3.13490</td>\n",
       "      <td>0.34073</td>\n",
       "      <td>0.58489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4.30640</td>\n",
       "      <td>8.20680</td>\n",
       "      <td>-2.78240</td>\n",
       "      <td>-1.43360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>-2.79140</td>\n",
       "      <td>1.77340</td>\n",
       "      <td>6.77560</td>\n",
       "      <td>-0.39915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>2.65620</td>\n",
       "      <td>10.70440</td>\n",
       "      <td>-3.30850</td>\n",
       "      <td>-4.07670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-1.80300</td>\n",
       "      <td>11.88180</td>\n",
       "      <td>2.04580</td>\n",
       "      <td>-5.27280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1.56910</td>\n",
       "      <td>6.34650</td>\n",
       "      <td>-0.18280</td>\n",
       "      <td>-2.40990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1        2        3\n",
       "678  2.46730   1.39260  1.71250  0.41421\n",
       "505  2.56500   8.63300 -2.99410 -1.30820\n",
       "30  -0.24811  -0.17797  4.90680  0.15429\n",
       "828 -2.59120  -0.10554  1.27980  1.04140\n",
       "396  2.67990   3.13490  0.34073  0.58489\n",
       "..       ...       ...      ...      ...\n",
       "92   4.30640   8.20680 -2.78240 -1.43360\n",
       "593 -2.79140   1.77340  6.77560 -0.39915\n",
       "655  2.65620  10.70440 -3.30850 -4.07670\n",
       "168 -1.80300  11.88180  2.04580 -5.27280\n",
       "430  1.56910   6.34650 -0.18280 -2.40990\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(b) Repeat each of the following two procedures 50 times. You will have 50 errors for\n",
    "90 SVMs per each procedure.\n",
    "\n",
    "i. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set using linear kernel and L1 penalty. Select the penalty parameter using\n",
    "5-fold cross validation.4 Repeat this process by adding 10 other randomly \n",
    "selected data points to the pool, until you use all the 900 points. Do NOT\n",
    "replace the samples back into the training set at each step. Calculate the\n",
    "test error for each SVM. You will have 90 SVMs that were trained using 10,\n",
    "20, 30, ... , 900 data points and their 90 test errors. You have implemented\n",
    "passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:21<00:00,  5.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.165254</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.328390</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.082627  0.038136  0.014831  0.021186  0.010593  0.010593  0.027542   \n",
       "1   0.101695  0.076271  0.076271  0.044492  0.065678  0.044492  0.044492   \n",
       "2   0.110169  0.084746  0.072034  0.042373  0.036017  0.046610  0.040254   \n",
       "3   0.131356  0.088983  0.057203  0.063559  0.023305  0.027542  0.029661   \n",
       "4   0.122881  0.076271  0.029661  0.069915  0.042373  0.036017  0.025424   \n",
       "5   0.046610  0.012712  0.023305  0.012712  0.012712  0.012712  0.027542   \n",
       "6   0.095339  0.112288  0.135593  0.133475  0.063559  0.036017  0.036017   \n",
       "7   0.082627  0.014831  0.016949  0.016949  0.012712  0.012712  0.012712   \n",
       "8   0.080508  0.012712  0.016949  0.019068  0.012712  0.016949  0.016949   \n",
       "9   0.095339  0.031780  0.023305  0.023305  0.008475  0.008475  0.008475   \n",
       "10  0.122881  0.091102  0.061441  0.019068  0.023305  0.019068  0.019068   \n",
       "11  0.129237  0.125000  0.014831  0.023305  0.027542  0.016949  0.019068   \n",
       "12  0.029661  0.008475  0.008475  0.012712  0.012712  0.012712  0.012712   \n",
       "13  0.120763  0.122881  0.108051  0.042373  0.040254  0.063559  0.038136   \n",
       "14  0.222458  0.033898  0.040254  0.040254  0.012712  0.012712  0.021186   \n",
       "15  0.116525  0.031780  0.019068  0.019068  0.027542  0.019068  0.016949   \n",
       "16  0.463983  0.103814  0.031780  0.029661  0.029661  0.029661  0.016949   \n",
       "17  0.463983  0.014831  0.012712  0.012712  0.014831  0.014831  0.012712   \n",
       "18  0.114407  0.084746  0.095339  0.052966  0.063559  0.016949  0.019068   \n",
       "19  0.033898  0.036017  0.012712  0.012712  0.012712  0.016949  0.012712   \n",
       "20  0.021186  0.023305  0.029661  0.025424  0.019068  0.012712  0.012712   \n",
       "21  0.120763  0.067797  0.036017  0.033898  0.027542  0.023305  0.023305   \n",
       "22  0.463983  0.048729  0.023305  0.033898  0.023305  0.023305  0.036017   \n",
       "23  0.381356  0.133475  0.103814  0.021186  0.021186  0.025424  0.029661   \n",
       "24  0.463983  0.144068  0.088983  0.052966  0.021186  0.029661  0.012712   \n",
       "25  0.165254  0.082627  0.023305  0.059322  0.120763  0.057203  0.063559   \n",
       "26  0.133475  0.082627  0.067797  0.057203  0.033898  0.012712  0.023305   \n",
       "27  0.023305  0.082627  0.046610  0.036017  0.025424  0.025424  0.010593   \n",
       "28  0.029661  0.036017  0.046610  0.046610  0.019068  0.038136  0.023305   \n",
       "29  0.076271  0.057203  0.067797  0.074153  0.065678  0.040254  0.040254   \n",
       "30  0.133475  0.025424  0.044492  0.048729  0.027542  0.012712  0.012712   \n",
       "31  0.135593  0.080508  0.061441  0.019068  0.019068  0.016949  0.012712   \n",
       "32  0.033898  0.031780  0.036017  0.036017  0.038136  0.029661  0.029661   \n",
       "33  0.025424  0.019068  0.078390  0.019068  0.016949  0.016949  0.016949   \n",
       "34  0.328390  0.036017  0.061441  0.031780  0.027542  0.029661  0.029661   \n",
       "35  0.036017  0.029661  0.029661  0.025424  0.025424  0.029661  0.027542   \n",
       "36  0.463983  0.036017  0.031780  0.036017  0.016949  0.019068  0.016949   \n",
       "37  0.148305  0.099576  0.010593  0.052966  0.038136  0.019068  0.014831   \n",
       "38  0.084746  0.033898  0.038136  0.027542  0.027542  0.025424  0.021186   \n",
       "39  0.415254  0.025424  0.069915  0.014831  0.012712  0.019068  0.014831   \n",
       "40  0.146186  0.050847  0.057203  0.065678  0.031780  0.031780  0.023305   \n",
       "41  0.101695  0.082627  0.016949  0.042373  0.042373  0.008475  0.012712   \n",
       "42  0.112288  0.012712  0.029661  0.016949  0.012712  0.012712  0.012712   \n",
       "43  0.029661  0.025424  0.025424  0.027542  0.014831  0.014831  0.027542   \n",
       "44  0.154661  0.052966  0.044492  0.036017  0.029661  0.029661  0.029661   \n",
       "45  0.042373  0.019068  0.055085  0.012712  0.012712  0.012712  0.014831   \n",
       "46  0.125000  0.141949  0.080508  0.046610  0.086864  0.016949  0.021186   \n",
       "47  0.082627  0.055085  0.027542  0.025424  0.012712  0.012712  0.016949   \n",
       "48  0.122881  0.154661  0.025424  0.014831  0.069915  0.033898  0.031780   \n",
       "49  0.110169  0.072034  0.063559  0.027542  0.023305  0.012712  0.012712   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.027542  0.012712  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "1   0.044492  0.019068  0.019068  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "2   0.027542  0.027542  0.027542  ...  0.008475  0.008475  0.008475  0.012712   \n",
       "3   0.027542  0.025424  0.025424  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "4   0.008475  0.025424  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "5   0.025424  0.023305  0.023305  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "6   0.012712  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "7   0.010593  0.008475  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "8   0.012712  0.010593  0.008475  ...  0.008475  0.012712  0.012712  0.012712   \n",
       "9   0.008475  0.008475  0.012712  ...  0.008475  0.008475  0.006356  0.006356   \n",
       "10  0.012712  0.012712  0.010593  ...  0.012712  0.008475  0.008475  0.008475   \n",
       "11  0.019068  0.016949  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "12  0.012712  0.012712  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "13  0.021186  0.023305  0.019068  ...  0.006356  0.012712  0.012712  0.012712   \n",
       "14  0.012712  0.021186  0.008475  ...  0.012712  0.012712  0.008475  0.008475   \n",
       "15  0.016949  0.012712  0.021186  ...  0.012712  0.008475  0.008475  0.008475   \n",
       "16  0.016949  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "17  0.016949  0.012712  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "18  0.019068  0.021186  0.023305  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "19  0.012712  0.014831  0.016949  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "20  0.021186  0.014831  0.012712  ...  0.008475  0.008475  0.016949  0.014831   \n",
       "21  0.023305  0.023305  0.023305  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "22  0.023305  0.023305  0.023305  ...  0.008475  0.008475  0.012712  0.008475   \n",
       "23  0.019068  0.021186  0.029661  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "24  0.021186  0.025424  0.025424  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "25  0.055085  0.016949  0.016949  ...  0.008475  0.008475  0.012712  0.012712   \n",
       "26  0.021186  0.031780  0.014831  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "27  0.021186  0.021186  0.016949  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "28  0.023305  0.016949  0.016949  ...  0.006356  0.008475  0.008475  0.008475   \n",
       "29  0.040254  0.038136  0.023305  ...  0.012712  0.008475  0.008475  0.008475   \n",
       "30  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.008475  0.008475   \n",
       "31  0.012712  0.012712  0.006356  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "32  0.029661  0.029661  0.016949  ...  0.008475  0.008475  0.006356  0.008475   \n",
       "33  0.016949  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "34  0.031780  0.029661  0.025424  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "35  0.014831  0.027542  0.019068  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "36  0.016949  0.023305  0.006356  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "37  0.014831  0.027542  0.019068  ...  0.012712  0.012712  0.008475  0.008475   \n",
       "38  0.025424  0.023305  0.019068  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "39  0.014831  0.016949  0.014831  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "40  0.023305  0.023305  0.023305  ...  0.008475  0.012712  0.008475  0.012712   \n",
       "41  0.012712  0.012712  0.012712  ...  0.006356  0.006356  0.006356  0.008475   \n",
       "42  0.012712  0.012712  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "43  0.025424  0.023305  0.023305  ...  0.008475  0.014831  0.008475  0.008475   \n",
       "44  0.016949  0.016949  0.016949  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "45  0.008475  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "46  0.025424  0.025424  0.025424  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "47  0.012712  0.016949  0.012712  ...  0.006356  0.008475  0.006356  0.006356   \n",
       "48  0.025424  0.027542  0.023305  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "49  0.012712  0.012712  0.016949  ...  0.012712  0.012712  0.008475  0.008475   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "1   0.008475  0.012712  0.012712  0.012712  0.008475  0.008475  \n",
       "2   0.012712  0.012712  0.012712  0.008475  0.008475  0.008475  \n",
       "3   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "4   0.008475  0.008475  0.008475  0.008475  0.006356  0.006356  \n",
       "5   0.008475  0.012712  0.012712  0.012712  0.008475  0.008475  \n",
       "6   0.010593  0.008475  0.012712  0.012712  0.012712  0.006356  \n",
       "7   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "8   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "9   0.006356  0.006356  0.012712  0.006356  0.008475  0.008475  \n",
       "10  0.008475  0.008475  0.008475  0.012712  0.008475  0.008475  \n",
       "11  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "12  0.008475  0.006356  0.008475  0.008475  0.008475  0.008475  \n",
       "13  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "14  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "15  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "16  0.012712  0.012712  0.012712  0.008475  0.008475  0.008475  \n",
       "17  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "18  0.008475  0.008475  0.008475  0.008475  0.008475  0.012712  \n",
       "19  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "20  0.012712  0.014831  0.014831  0.008475  0.008475  0.008475  \n",
       "21  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "22  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "23  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "24  0.016949  0.016949  0.008475  0.008475  0.008475  0.008475  \n",
       "25  0.012712  0.008475  0.012712  0.008475  0.008475  0.008475  \n",
       "26  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "27  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "28  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "29  0.012712  0.012712  0.012712  0.008475  0.008475  0.008475  \n",
       "30  0.008475  0.008475  0.012712  0.008475  0.008475  0.012712  \n",
       "31  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "32  0.008475  0.008475  0.008475  0.008475  0.012712  0.008475  \n",
       "33  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "34  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "35  0.008475  0.008475  0.008475  0.016949  0.012712  0.008475  \n",
       "36  0.008475  0.012712  0.008475  0.008475  0.008475  0.008475  \n",
       "37  0.012712  0.012712  0.008475  0.008475  0.008475  0.008475  \n",
       "38  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "39  0.012712  0.012712  0.008475  0.008475  0.012712  0.012712  \n",
       "40  0.012712  0.012712  0.012712  0.012712  0.012712  0.006356  \n",
       "41  0.008475  0.008475  0.008475  0.012712  0.012712  0.008475  \n",
       "42  0.008475  0.008475  0.016949  0.016949  0.008475  0.008475  \n",
       "43  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "44  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "45  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "46  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "47  0.006356  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "48  0.008475  0.006356  0.008475  0.006356  0.012712  0.008475  \n",
       "49  0.008475  0.012712  0.012712  0.012712  0.012712  0.006356  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def passive_learning(train_x, train_y, test_x, test_y):\n",
    "    x_select, y_select = pd.DataFrame(), pd.DataFrame()\n",
    "    random_ids = np.random.randint(0, len(train_x), 10)\n",
    "    x_select, y_select = train_x.iloc[random_ids], train_y.iloc[random_ids]\n",
    "    y_ = pd.DataFrame(y_select)\n",
    "    while len(y_[y_[4]==1])==0 or len(y_[y_[4]==0])==0:\n",
    "        random_ids = np.random.randint(0, len(train_x), 10)\n",
    "        x_select, y_select = train_x.iloc[random_ids], train_y.iloc[random_ids]\n",
    "        y_ = pd.DataFrame(y_select)\n",
    "    # print(y_select)\n",
    "    remain_x, remain_y = train_x.drop(train_x.index[random_ids]), train_y.drop(train_x.index[random_ids])\n",
    "    x_select = x_select.reset_index(drop=True)\n",
    "    y_select = y_select.reset_index(drop=True)\n",
    "    remain_x = remain_x.reset_index(drop=True)\n",
    "    remain_y = remain_y.reset_index(drop=True)\n",
    "\n",
    "    error=[]\n",
    "    for i in range(90):\n",
    "        # print(i, len())\n",
    "        parameters = {'C': [10**c for c in np.arange(-4, 4, dtype=float)]}\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        clf = GridSearchCV(svc, parameters, cv=5, n_jobs=-1)\n",
    "        clf.fit(x_select, y_select)\n",
    "        best_C = clf.best_params_['C']\n",
    "\n",
    "        svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "        svc.fit(x_select, y_select)\n",
    "        error.append(1-svc.score(test_x, test_y))\n",
    "\n",
    "        if i==89: break\n",
    "        add_x = remain_x.sample(n=10, axis=0)\n",
    "        ids = list(add_x.index)\n",
    "        x_select = pd.concat([x_select, remain_x.iloc[ids]], axis=0)\n",
    "        y_select = pd.concat([y_select, remain_y.iloc[ids]], axis=0)\n",
    "\n",
    "        remain_x = remain_x.drop(ids)\n",
    "        remain_y = remain_y.drop(ids)\n",
    "        remain_x = remain_x.reset_index(drop=True)\n",
    "        remain_y = remain_y.reset_index(drop=True)\n",
    "\n",
    "    return error\n",
    "\n",
    "passive_errors = []\n",
    "for i in tqdm(range(50)):\n",
    "    passive_errors.append(passive_learning(trainX, trainY, testX, testY))\n",
    "pd.DataFrame(passive_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ii. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set5 using linear kernel and L1 penalty. Select the parameters of the SVM\n",
    "with 5-fold cross validation. Choose the 10 closest data points in the training\n",
    "set to the hyperplane of the SVM6 and add them to the pool. Do not replace\n",
    "the samples back into the training set. Train a new SVM using the pool.\n",
    "Repeat this process until all training data is used. You will have 90 SVMs\n",
    "that were trained using 10, 20, 30,..., 900 data points and their 90 test errors.\n",
    "You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:16<00:00,  5.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.241525</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.264831</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.218220</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.463983</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.292373</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.226695</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.463983  0.080508  0.027542  0.014831  0.006356  0.010593  0.010593   \n",
       "1   0.135593  0.023305  0.023305  0.048729  0.019068  0.014831  0.016949   \n",
       "2   0.241525  0.050847  0.008475  0.046610  0.036017  0.014831  0.023305   \n",
       "3   0.065678  0.052966  0.040254  0.019068  0.010593  0.008475  0.006356   \n",
       "4   0.125000  0.078390  0.061441  0.023305  0.025424  0.021186  0.006356   \n",
       "5   0.127119  0.095339  0.027542  0.012712  0.019068  0.012712  0.008475   \n",
       "6   0.135593  0.019068  0.031780  0.008475  0.008475  0.008475  0.008475   \n",
       "7   0.127119  0.016949  0.059322  0.014831  0.012712  0.008475  0.008475   \n",
       "8   0.161017  0.086864  0.040254  0.021186  0.016949  0.016949  0.014831   \n",
       "9   0.065678  0.029661  0.019068  0.021186  0.012712  0.008475  0.008475   \n",
       "10  0.122881  0.025424  0.082627  0.036017  0.008475  0.008475  0.008475   \n",
       "11  0.038136  0.019068  0.264831  0.061441  0.019068  0.014831  0.008475   \n",
       "12  0.139831  0.036017  0.019068  0.006356  0.010593  0.006356  0.006356   \n",
       "13  0.127119  0.033898  0.021186  0.025424  0.012712  0.008475  0.008475   \n",
       "14  0.180085  0.069915  0.033898  0.012712  0.012712  0.006356  0.008475   \n",
       "15  0.065678  0.029661  0.012712  0.036017  0.008475  0.012712  0.010593   \n",
       "16  0.105932  0.036017  0.029661  0.061441  0.044492  0.033898  0.029661   \n",
       "17  0.078390  0.036017  0.019068  0.010593  0.006356  0.008475  0.008475   \n",
       "18  0.050847  0.016949  0.014831  0.012712  0.006356  0.012712  0.006356   \n",
       "19  0.131356  0.033898  0.021186  0.036017  0.008475  0.016949  0.008475   \n",
       "20  0.042373  0.012712  0.038136  0.014831  0.012712  0.008475  0.006356   \n",
       "21  0.108051  0.023305  0.031780  0.012712  0.008475  0.008475  0.008475   \n",
       "22  0.131356  0.031780  0.061441  0.027542  0.027542  0.027542  0.025424   \n",
       "23  0.116525  0.042373  0.046610  0.029661  0.012712  0.014831  0.008475   \n",
       "24  0.218220  0.016949  0.014831  0.008475  0.008475  0.008475  0.002119   \n",
       "25  0.194915  0.072034  0.021186  0.027542  0.027542  0.019068  0.014831   \n",
       "26  0.135593  0.027542  0.046610  0.010593  0.008475  0.008475  0.008475   \n",
       "27  0.118644  0.048729  0.012712  0.012712  0.016949  0.008475  0.006356   \n",
       "28  0.061441  0.069915  0.023305  0.008475  0.057203  0.008475  0.008475   \n",
       "29  0.086864  0.038136  0.023305  0.059322  0.036017  0.025424  0.019068   \n",
       "30  0.122881  0.063559  0.036017  0.012712  0.014831  0.014831  0.012712   \n",
       "31  0.091102  0.027542  0.008475  0.023305  0.016949  0.016949  0.006356   \n",
       "32  0.008475  0.137712  0.067797  0.027542  0.010593  0.008475  0.008475   \n",
       "33  0.135593  0.127119  0.031780  0.019068  0.010593  0.021186  0.006356   \n",
       "34  0.463983  0.082627  0.027542  0.029661  0.014831  0.029661  0.019068   \n",
       "35  0.292373  0.082627  0.061441  0.025424  0.027542  0.021186  0.012712   \n",
       "36  0.118644  0.044492  0.027542  0.016949  0.006356  0.012712  0.012712   \n",
       "37  0.044492  0.027542  0.025424  0.012712  0.010593  0.006356  0.006356   \n",
       "38  0.127119  0.025424  0.050847  0.048729  0.008475  0.014831  0.002119   \n",
       "39  0.012712  0.023305  0.016949  0.016949  0.006356  0.012712  0.002119   \n",
       "40  0.072034  0.038136  0.042373  0.008475  0.008475  0.008475  0.008475   \n",
       "41  0.133475  0.055085  0.019068  0.008475  0.008475  0.006356  0.012712   \n",
       "42  0.110169  0.033898  0.046610  0.033898  0.006356  0.012712  0.010593   \n",
       "43  0.025424  0.019068  0.072034  0.016949  0.012712  0.012712  0.002119   \n",
       "44  0.025424  0.025424  0.012712  0.008475  0.008475  0.006356  0.006356   \n",
       "45  0.190678  0.118644  0.025424  0.008475  0.008475  0.008475  0.008475   \n",
       "46  0.226695  0.012712  0.019068  0.008475  0.008475  0.008475  0.012712   \n",
       "47  0.025424  0.052966  0.016949  0.012712  0.012712  0.008475  0.008475   \n",
       "48  0.148305  0.067797  0.084746  0.021186  0.010593  0.012712  0.012712   \n",
       "49  0.093220  0.055085  0.038136  0.016949  0.012712  0.010593  0.008475   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.008475  0.008475  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "1   0.012712  0.008475  0.006356  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "2   0.008475  0.008475  0.008475  ...  0.012712  0.006356  0.008475  0.008475   \n",
       "3   0.002119  0.006356  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "4   0.012712  0.008475  0.008475  ...  0.008475  0.008475  0.006356  0.006356   \n",
       "5   0.008475  0.006356  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "6   0.008475  0.006356  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "7   0.006356  0.008475  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "8   0.008475  0.012712  0.008475  ...  0.012712  0.012712  0.008475  0.008475   \n",
       "9   0.008475  0.008475  0.012712  ...  0.008475  0.008475  0.008475  0.006356   \n",
       "10  0.008475  0.006356  0.008475  ...  0.012712  0.012712  0.012712  0.008475   \n",
       "11  0.012712  0.002119  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "12  0.006356  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "13  0.012712  0.008475  0.008475  ...  0.012712  0.012712  0.012712  0.006356   \n",
       "14  0.006356  0.002119  0.012712  ...  0.012712  0.006356  0.006356  0.006356   \n",
       "15  0.008475  0.008475  0.012712  ...  0.012712  0.012712  0.012712  0.008475   \n",
       "16  0.012712  0.006356  0.006356  ...  0.008475  0.008475  0.012712  0.012712   \n",
       "17  0.008475  0.012712  0.012712  ...  0.012712  0.008475  0.008475  0.008475   \n",
       "18  0.006356  0.002119  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "19  0.012712  0.012712  0.008475  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "20  0.012712  0.002119  0.012712  ...  0.006356  0.012712  0.012712  0.006356   \n",
       "21  0.012712  0.006356  0.008475  ...  0.012712  0.012712  0.008475  0.008475   \n",
       "22  0.006356  0.012712  0.008475  ...  0.012712  0.006356  0.006356  0.006356   \n",
       "23  0.008475  0.008475  0.012712  ...  0.008475  0.006356  0.012712  0.008475   \n",
       "24  0.002119  0.008475  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "25  0.006356  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.008475   \n",
       "26  0.006356  0.002119  0.008475  ...  0.012712  0.006356  0.006356  0.006356   \n",
       "27  0.008475  0.008475  0.012712  ...  0.008475  0.008475  0.006356  0.008475   \n",
       "28  0.008475  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "29  0.008475  0.008475  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "30  0.012712  0.002119  0.002119  ...  0.012712  0.012712  0.012712  0.006356   \n",
       "31  0.002119  0.002119  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "32  0.012712  0.006356  0.012712  ...  0.012712  0.012712  0.006356  0.006356   \n",
       "33  0.012712  0.006356  0.002119  ...  0.012712  0.006356  0.012712  0.012712   \n",
       "34  0.014831  0.012712  0.002119  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "35  0.006356  0.002119  0.002119  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "36  0.006356  0.002119  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "37  0.002119  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "38  0.008475  0.012712  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "39  0.012712  0.002119  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "40  0.002119  0.008475  0.008475  ...  0.006356  0.006356  0.006356  0.006356   \n",
       "41  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "42  0.006356  0.012712  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "43  0.012712  0.012712  0.006356  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "44  0.008475  0.008475  0.008475  ...  0.012712  0.006356  0.012712  0.006356   \n",
       "45  0.006356  0.012712  0.006356  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "46  0.008475  0.008475  0.008475  ...  0.012712  0.006356  0.006356  0.006356   \n",
       "47  0.008475  0.012712  0.012712  ...  0.008475  0.008475  0.008475  0.008475   \n",
       "48  0.006356  0.002119  0.012712  ...  0.012712  0.012712  0.006356  0.008475   \n",
       "49  0.008475  0.002119  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "1   0.012712  0.012712  0.012712  0.012712  0.012712  0.008475  \n",
       "2   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "3   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "4   0.012712  0.006356  0.006356  0.008475  0.008475  0.008475  \n",
       "5   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "6   0.012712  0.012712  0.012712  0.012712  0.012712  0.008475  \n",
       "7   0.012712  0.012712  0.012712  0.012712  0.008475  0.008475  \n",
       "8   0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "9   0.006356  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "10  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "11  0.008475  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "12  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "13  0.006356  0.006356  0.006356  0.008475  0.006356  0.006356  \n",
       "14  0.012712  0.006356  0.006356  0.012712  0.012712  0.012712  \n",
       "15  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "16  0.012712  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "17  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "18  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "19  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "20  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "21  0.008475  0.008475  0.008475  0.008475  0.008475  0.006356  \n",
       "22  0.006356  0.006356  0.006356  0.008475  0.008475  0.008475  \n",
       "23  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "24  0.012712  0.012712  0.012712  0.008475  0.008475  0.008475  \n",
       "25  0.012712  0.008475  0.006356  0.012712  0.012712  0.012712  \n",
       "26  0.006356  0.006356  0.006356  0.008475  0.008475  0.008475  \n",
       "27  0.006356  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "28  0.008475  0.008475  0.006356  0.006356  0.006356  0.006356  \n",
       "29  0.012712  0.012712  0.006356  0.008475  0.008475  0.008475  \n",
       "30  0.012712  0.012712  0.008475  0.012712  0.012712  0.012712  \n",
       "31  0.012712  0.012712  0.012712  0.012712  0.008475  0.008475  \n",
       "32  0.006356  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "33  0.012712  0.012712  0.012712  0.012712  0.008475  0.008475  \n",
       "34  0.012712  0.012712  0.012712  0.012712  0.012712  0.008475  \n",
       "35  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "36  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "37  0.012712  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "38  0.008475  0.008475  0.006356  0.008475  0.006356  0.008475  \n",
       "39  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "40  0.006356  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "41  0.012712  0.012712  0.012712  0.012712  0.008475  0.008475  \n",
       "42  0.012712  0.012712  0.008475  0.012712  0.012712  0.008475  \n",
       "43  0.012712  0.008475  0.008475  0.008475  0.006356  0.008475  \n",
       "44  0.006356  0.006356  0.006356  0.008475  0.008475  0.008475  \n",
       "45  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "46  0.006356  0.006356  0.006356  0.012712  0.012712  0.008475  \n",
       "47  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "48  0.008475  0.008475  0.008475  0.008475  0.008475  0.008475  \n",
       "49  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def active_learning(train_x, train_y, test_x, test_y):\n",
    "    x_select, y_select = pd.DataFrame(), pd.DataFrame()\n",
    "    random_ids = np.random.randint(0, len(train_x), 10)\n",
    "    x_select, y_select = train_x.iloc[random_ids], train_y.iloc[random_ids]\n",
    "    y_ = pd.DataFrame(y_select)\n",
    "    while len(y_[y_[4]==1])==0 or len(y_[y_[4]==0])==0:\n",
    "        random_ids = np.random.randint(0, len(train_x), 10)\n",
    "        x_select, y_select = train_x.iloc[random_ids], train_y.iloc[random_ids]\n",
    "        y_ = pd.DataFrame(y_select)\n",
    "    remain_x, remain_y = train_x.drop(train_x.index[random_ids]), train_y.drop(train_x.index[random_ids])\n",
    "    x_select = x_select.reset_index(drop=True)\n",
    "    y_select = y_select.reset_index(drop=True)\n",
    "    remain_x = remain_x.reset_index(drop=True)\n",
    "    remain_y = remain_y.reset_index(drop=True)\n",
    "\n",
    "    error=[]\n",
    "    for i in range(90):\n",
    "        parameters = {'C': [10**c for c in np.arange(-4, 4, dtype=float)]}\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        clf = GridSearchCV(svc, parameters, cv=5, n_jobs=-1)\n",
    "        clf.fit(x_select, y_select)\n",
    "        best_C = clf.best_params_['C']\n",
    "\n",
    "        svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "        svc.fit(x_select, y_select)\n",
    "        error.append(1-svc.score(test_x, test_y))\n",
    "\n",
    "        if i==89: break\n",
    "        dis = svc.decision_function(remain_x)\n",
    "        top_10_ids = np.argsort(abs(dis))[:10]\n",
    "        x_select = pd.concat([x_select, remain_x.iloc[top_10_ids]], axis=0)\n",
    "        y_select = pd.concat([y_select, remain_y.iloc[top_10_ids]], axis=0)\n",
    "        remain_x = remain_x.drop(top_10_ids)\n",
    "        remain_y = remain_y.drop(top_10_ids)\n",
    "        remain_x = remain_x.reset_index(drop=True)\n",
    "        remain_y = remain_y.reset_index(drop=True)\n",
    "\n",
    "    return error\n",
    "\n",
    "active_errors = []\n",
    "for i in tqdm(range(50)):\n",
    "    active_errors.append(active_learning(trainX, trainY, testX, testY))\n",
    "pd.DataFrame(active_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i\n",
    "and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot\n",
    "average test error versus number of training instances for both active and passive\n",
    "learners on the same figure and report your conclusions. Here, you are actually\n",
    "obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i*10 for i in range(1,91)], pd.DataFrame(active_errors).mean(), c='r', label='Active Learning')\n",
    "plt.plot([i*10 for i in range(1,91)], pd.DataFrame(passive_errors).mean(), c='b', label='Passive Learning')\n",
    "plt.xlabel('Number of Traning Instances')\n",
    "plt.ylabel('Test Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My conclusion is when train data increases, the test error decreases much faster using active learning, compared to passive leaerning. Active learning can achieve a better result when we have limited label ability."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "652856dbc3ab2a5d901ecb0bf4a112f6ee4b44e9af7f37c5c2559b5777fde759"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
